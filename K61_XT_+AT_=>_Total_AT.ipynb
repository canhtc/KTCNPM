{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XT +AT => Total AT",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/canhtc/KTCNPM/blob/master/K61_XT_%2BAT_%3D%3E_Total_AT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGE_12GJo9f8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88d08591-177e-49e3-d2b1-c33d2fb8803c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/driver')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/driver; to attempt to forcibly remount, call drive.mount(\"/content/driver\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZgyetqPo_Rv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d836206e-d0cc-44d9-dfba-4608dd348134"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from statsmodels.tools.eval_measures import rmse\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from tensorflow import keras\n",
        "from glob import glob\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pemg1EAIntml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_train = '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/*.csv'\n",
        "path_test = '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/test_k59/dataH.csv'\n",
        "\n",
        "# path_train = '/content/driver/My Drive/Colab data/ktcnpm_data/k60/datacsv/train/*.csv'\n",
        "# path_test = '/content/driver/My Drive/Colab data/ktcnpm_data/k60/datacsv/test/data_6.csv'\n",
        "\n",
        "# path_train = '/content/driver/My Drive/Colab data/ktcnpm_data/k61/train/*.csv'\n",
        "# path_test = '/content/driver/My Drive/Colab data/ktcnpm_data/k61/test/data_10.csv'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYcDbhUWpDDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "ded9ab83-e686-4667-8a3d-b1ae24ae8371"
      },
      "source": [
        "\n",
        "\n",
        "files = glob(path_train)\n",
        "x_input =[]\n",
        "y_output=[]\n",
        "print(files)\n",
        "for file in files:\n",
        "  data = pd.read_csv(file)\n",
        "  data_xt = data['XT'].values\n",
        "  data_at = data['AT'].values\n",
        "  data_total_at = data['TOTAL_AT'].values\n",
        "\n",
        "  # data = []\n",
        "  # data.append(data_xt)\n",
        "  # data.append(data_ac)\n",
        "  # data.append(data_total_ac)\n",
        "\n",
        "  # data = np.asarray(data)\n",
        "\n",
        "  # scaler = StandardScaler()\n",
        "\n",
        "  \n",
        "  # data = scaler.fit_transform(data)\n",
        "\n",
        "  # data_xt = data[0]\n",
        "  # data_ac = data[1]\n",
        "  # data_total_ac = data[2]\n",
        "\n",
        "  # print(data_total_at)\n",
        "  # scaler = MinMaxScaler()\n",
        "  \n",
        "\n",
        "#dung de lay du lieu theo cap\n",
        "  data_input2D = []\n",
        "  data_output1D = []\n",
        "  steps = 3\n",
        "  \n",
        "  for i in range(0,len(data_xt)-steps+1):\n",
        "    temp_3D = []\n",
        "    for k in range(i, i+steps):\n",
        "      temp_2D = []\n",
        "      temp_2D.append(data_xt[k])\n",
        "      temp_2D.append(data_at[k])\n",
        "      \n",
        "      temp_3D.append(temp_2D)\n",
        "\n",
        "    data_output1D.append(data_total_at[i])\n",
        "\n",
        "    data_input2D.append(temp_3D)\n",
        "  x_input.append(data_input2D)\n",
        "  y_output.append(data_output1D)\n",
        "\n",
        "x_input = np.concatenate( x_input, axis=0 )\n",
        "y_output = np.concatenate( y_output, axis=0 )\n",
        "x_train = np.asarray(x_input)\n",
        "y_train = np.asarray(y_output)\n",
        "\n",
        "x_train = x_train.reshape(-1,3,2)\n",
        "y_train = y_train.reshape(-1,1)\n",
        "print(x_train[:10], y_train[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataA.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataB.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataC.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataD.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataE.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataF.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataG.csv']\n",
            "[[[ 0.13  4.  ]\n",
            "  [ 0.17  5.  ]\n",
            "  [ 0.24  6.  ]]\n",
            "\n",
            " [[ 0.17  5.  ]\n",
            "  [ 0.24  6.  ]\n",
            "  [ 0.29  7.  ]]\n",
            "\n",
            " [[ 0.24  6.  ]\n",
            "  [ 0.29  7.  ]\n",
            "  [ 0.35  8.  ]]\n",
            "\n",
            " [[ 0.29  7.  ]\n",
            "  [ 0.35  8.  ]\n",
            "  [ 0.44  9.  ]]\n",
            "\n",
            " [[ 0.35  8.  ]\n",
            "  [ 0.44  9.  ]\n",
            "  [ 0.51 10.  ]]\n",
            "\n",
            " [[ 0.44  9.  ]\n",
            "  [ 0.51 10.  ]\n",
            "  [ 0.59 11.  ]]\n",
            "\n",
            " [[ 0.51 10.  ]\n",
            "  [ 0.59 11.  ]\n",
            "  [ 0.68 12.  ]]\n",
            "\n",
            " [[ 0.59 11.  ]\n",
            "  [ 0.68 12.  ]\n",
            "  [ 0.75 13.  ]]\n",
            "\n",
            " [[ 0.68 12.  ]\n",
            "  [ 0.75 13.  ]\n",
            "  [ 0.81 14.  ]]\n",
            "\n",
            " [[ 0.75 13.  ]\n",
            "  [ 0.81 14.  ]\n",
            "  [ 0.86 15.  ]]] [[22]\n",
            " [22]\n",
            " [22]\n",
            " [22]\n",
            " [22]\n",
            " [22]\n",
            " [22]\n",
            " [22]\n",
            " [22]\n",
            " [22]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gzqAZa8pFNw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "168a01c9-71c9-4816-dfdf-d5e468b62d64"
      },
      "source": [
        "n_input = 3\n",
        "n_features = 2\n",
        "units = 10\n",
        "model = Sequential()\n",
        "model.add(LSTM(units, activation='relu', input_shape=(n_input, n_features),return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(units, activation='relu', return_sequences = True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(units, activation='relu', return_sequences = False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(units = 1))\n",
        "model.summary()\n",
        "adam = Adam(lr=0.001) \n",
        "model.compile(optimizer=adam, loss='mse')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 3, 10)             520       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 3, 10)             0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 3, 10)             840       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 10)             0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 10)                840       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,211\n",
            "Trainable params: 2,211\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hO1MD5mpHZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17c599be-75fa-475a-adf2-7cf358a9e962"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=1000, validation_split=0.2, verbose=1, batch_size=3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 72 samples, validate on 19 samples\n",
            "Epoch 1/1000\n",
            "72/72 [==============================] - 1s 17ms/step - loss: 350.5194 - val_loss: 371.1223\n",
            "Epoch 2/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 344.8963 - val_loss: 362.9593\n",
            "Epoch 3/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 332.8317 - val_loss: 338.2299\n",
            "Epoch 4/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 287.2407 - val_loss: 187.8825\n",
            "Epoch 5/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 207.8361 - val_loss: 73.1122\n",
            "Epoch 6/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 162.8280 - val_loss: 68.7292\n",
            "Epoch 7/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 180.5231 - val_loss: 95.9401\n",
            "Epoch 8/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 135.2716 - val_loss: 69.8756\n",
            "Epoch 9/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 116.7136 - val_loss: 33.9451\n",
            "Epoch 10/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 112.2254 - val_loss: 62.2413\n",
            "Epoch 11/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 137.4664 - val_loss: 26.7945\n",
            "Epoch 12/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 113.7270 - val_loss: 33.3655\n",
            "Epoch 13/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 76.4767 - val_loss: 18.4135\n",
            "Epoch 14/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 78.4363 - val_loss: 14.8757\n",
            "Epoch 15/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 99.1346 - val_loss: 14.2409\n",
            "Epoch 16/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 90.3955 - val_loss: 33.2408\n",
            "Epoch 17/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 81.4690 - val_loss: 19.8730\n",
            "Epoch 18/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 88.8673 - val_loss: 19.0047\n",
            "Epoch 19/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 96.7329 - val_loss: 23.1975\n",
            "Epoch 20/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 80.2634 - val_loss: 10.9418\n",
            "Epoch 21/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 82.9215 - val_loss: 35.4005\n",
            "Epoch 22/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 79.1688 - val_loss: 19.1129\n",
            "Epoch 23/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 87.1822 - val_loss: 17.4741\n",
            "Epoch 24/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 70.5491 - val_loss: 19.4169\n",
            "Epoch 25/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 61.7238 - val_loss: 9.2266\n",
            "Epoch 26/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 82.5737 - val_loss: 24.1135\n",
            "Epoch 27/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 88.7696 - val_loss: 15.4503\n",
            "Epoch 28/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 69.1779 - val_loss: 9.0211\n",
            "Epoch 29/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 76.2397 - val_loss: 5.5293\n",
            "Epoch 30/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 74.3064 - val_loss: 13.9953\n",
            "Epoch 31/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 80.0290 - val_loss: 11.5471\n",
            "Epoch 32/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 68.6160 - val_loss: 20.0340\n",
            "Epoch 33/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 91.5215 - val_loss: 10.9540\n",
            "Epoch 34/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 73.0428 - val_loss: 12.4791\n",
            "Epoch 35/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 63.2976 - val_loss: 5.9718\n",
            "Epoch 36/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 76.3418 - val_loss: 13.5435\n",
            "Epoch 37/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 60.1573 - val_loss: 8.7488\n",
            "Epoch 38/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 58.7982 - val_loss: 18.9144\n",
            "Epoch 39/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 56.2978 - val_loss: 28.6316\n",
            "Epoch 40/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 76.3716 - val_loss: 16.6578\n",
            "Epoch 41/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 56.7963 - val_loss: 22.1467\n",
            "Epoch 42/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 63.6217 - val_loss: 18.9782\n",
            "Epoch 43/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 73.1819 - val_loss: 13.2616\n",
            "Epoch 44/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 74.7294 - val_loss: 21.4692\n",
            "Epoch 45/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 45.0912 - val_loss: 18.6957\n",
            "Epoch 46/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 60.0424 - val_loss: 26.8873\n",
            "Epoch 47/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 62.7461 - val_loss: 34.5217\n",
            "Epoch 48/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 63.6125 - val_loss: 16.1028\n",
            "Epoch 49/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 61.2260 - val_loss: 17.1615\n",
            "Epoch 50/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 62.7306 - val_loss: 23.0236\n",
            "Epoch 51/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 51.3818 - val_loss: 23.5391\n",
            "Epoch 52/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 59.4103 - val_loss: 8.4077\n",
            "Epoch 53/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 57.0728 - val_loss: 7.0973\n",
            "Epoch 54/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 57.2098 - val_loss: 29.1777\n",
            "Epoch 55/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 59.9887 - val_loss: 15.2187\n",
            "Epoch 56/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 72.2524 - val_loss: 18.8747\n",
            "Epoch 57/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 54.6312 - val_loss: 25.9546\n",
            "Epoch 58/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 43.8779 - val_loss: 13.0861\n",
            "Epoch 59/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 50.0149 - val_loss: 22.7788\n",
            "Epoch 60/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 55.8804 - val_loss: 12.2911\n",
            "Epoch 61/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 56.8593 - val_loss: 14.6066\n",
            "Epoch 62/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 54.3956 - val_loss: 16.4927\n",
            "Epoch 63/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 49.9789 - val_loss: 13.9631\n",
            "Epoch 64/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 53.4258 - val_loss: 13.3803\n",
            "Epoch 65/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 36.6258 - val_loss: 33.3068\n",
            "Epoch 66/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 53.9918 - val_loss: 17.2564\n",
            "Epoch 67/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 65.3122 - val_loss: 18.9410\n",
            "Epoch 68/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 48.3255 - val_loss: 9.1336\n",
            "Epoch 69/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 63.4638 - val_loss: 21.3369\n",
            "Epoch 70/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 60.7692 - val_loss: 14.6431\n",
            "Epoch 71/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 50.4324 - val_loss: 23.3528\n",
            "Epoch 72/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 65.1590 - val_loss: 20.3382\n",
            "Epoch 73/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 46.6923 - val_loss: 14.0429\n",
            "Epoch 74/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 37.9127 - val_loss: 17.5289\n",
            "Epoch 75/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 52.8975 - val_loss: 24.0036\n",
            "Epoch 76/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 38.1919 - val_loss: 16.9464\n",
            "Epoch 77/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 55.1305 - val_loss: 29.2751\n",
            "Epoch 78/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 49.0205 - val_loss: 16.2607\n",
            "Epoch 79/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 57.4541 - val_loss: 15.5424\n",
            "Epoch 80/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 59.7488 - val_loss: 23.1077\n",
            "Epoch 81/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 63.2858 - val_loss: 20.8590\n",
            "Epoch 82/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 42.8537 - val_loss: 14.2755\n",
            "Epoch 83/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 52.4358 - val_loss: 21.6851\n",
            "Epoch 84/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 54.7688 - val_loss: 21.4349\n",
            "Epoch 85/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 64.2000 - val_loss: 14.9865\n",
            "Epoch 86/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 43.8289 - val_loss: 10.8818\n",
            "Epoch 87/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 55.5297 - val_loss: 13.5428\n",
            "Epoch 88/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 41.6699 - val_loss: 15.8982\n",
            "Epoch 89/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 57.1234 - val_loss: 20.9080\n",
            "Epoch 90/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 58.9917 - val_loss: 22.4172\n",
            "Epoch 91/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 60.5427 - val_loss: 20.8153\n",
            "Epoch 92/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 40.0020 - val_loss: 23.7621\n",
            "Epoch 93/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 45.6970 - val_loss: 24.8290\n",
            "Epoch 94/1000\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 65.7006 - val_loss: 16.1413\n",
            "Epoch 95/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 58.0964 - val_loss: 24.1030\n",
            "Epoch 96/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 51.6618 - val_loss: 18.8870\n",
            "Epoch 97/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 58.0856 - val_loss: 16.7706\n",
            "Epoch 98/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 44.8552 - val_loss: 27.1209\n",
            "Epoch 99/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 42.2023 - val_loss: 17.8547\n",
            "Epoch 100/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 49.9700 - val_loss: 23.0119\n",
            "Epoch 101/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 44.7947 - val_loss: 17.4845\n",
            "Epoch 102/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 37.2627 - val_loss: 18.9557\n",
            "Epoch 103/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 56.0839 - val_loss: 19.1126\n",
            "Epoch 104/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 41.3657 - val_loss: 25.1283\n",
            "Epoch 105/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 54.3116 - val_loss: 19.4599\n",
            "Epoch 106/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 49.4827 - val_loss: 11.2641\n",
            "Epoch 107/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 42.3198 - val_loss: 17.2021\n",
            "Epoch 108/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 53.2841 - val_loss: 27.4957\n",
            "Epoch 109/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 48.4367 - val_loss: 17.4612\n",
            "Epoch 110/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 46.9658 - val_loss: 27.3668\n",
            "Epoch 111/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 63.5388 - val_loss: 14.1281\n",
            "Epoch 112/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 68.1667 - val_loss: 9.7472\n",
            "Epoch 113/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 52.6291 - val_loss: 19.9702\n",
            "Epoch 114/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 53.5517 - val_loss: 17.9233\n",
            "Epoch 115/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 58.0104 - val_loss: 20.5360\n",
            "Epoch 116/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 56.0861 - val_loss: 10.7612\n",
            "Epoch 117/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 52.9345 - val_loss: 12.3315\n",
            "Epoch 118/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 40.3276 - val_loss: 16.3704\n",
            "Epoch 119/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 42.5516 - val_loss: 20.3484\n",
            "Epoch 120/1000\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 50.2500 - val_loss: 14.1659\n",
            "Epoch 121/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 44.5571 - val_loss: 30.0408\n",
            "Epoch 122/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 48.3665 - val_loss: 15.3705\n",
            "Epoch 123/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 49.2310 - val_loss: 11.1198\n",
            "Epoch 124/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 53.2504 - val_loss: 12.8028\n",
            "Epoch 125/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 44.6179 - val_loss: 9.4705\n",
            "Epoch 126/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 49.1748 - val_loss: 20.3220\n",
            "Epoch 127/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 42.6032 - val_loss: 13.3720\n",
            "Epoch 128/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 38.6993 - val_loss: 16.3945\n",
            "Epoch 129/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 55.5163 - val_loss: 9.5104\n",
            "Epoch 130/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 42.1129 - val_loss: 13.3322\n",
            "Epoch 131/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 43.0740 - val_loss: 14.1206\n",
            "Epoch 132/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 35.7217 - val_loss: 18.7275\n",
            "Epoch 133/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 41.0314 - val_loss: 23.8903\n",
            "Epoch 134/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 39.7850 - val_loss: 8.8969\n",
            "Epoch 135/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 52.5975 - val_loss: 11.8789\n",
            "Epoch 136/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 39.0608 - val_loss: 9.3203\n",
            "Epoch 137/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.2881 - val_loss: 17.8869\n",
            "Epoch 138/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 41.1269 - val_loss: 14.3975\n",
            "Epoch 139/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 38.3046 - val_loss: 10.7840\n",
            "Epoch 140/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 36.6519 - val_loss: 14.0672\n",
            "Epoch 141/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 36.9293 - val_loss: 9.1239\n",
            "Epoch 142/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 35.7350 - val_loss: 14.5421\n",
            "Epoch 143/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 37.1463 - val_loss: 12.6118\n",
            "Epoch 144/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.1798 - val_loss: 7.9918\n",
            "Epoch 145/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 45.1201 - val_loss: 13.2902\n",
            "Epoch 146/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 48.5120 - val_loss: 13.7210\n",
            "Epoch 147/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 49.4066 - val_loss: 14.2284\n",
            "Epoch 148/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 41.9045 - val_loss: 12.7530\n",
            "Epoch 149/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 39.3841 - val_loss: 11.2305\n",
            "Epoch 150/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.2080 - val_loss: 12.7580\n",
            "Epoch 151/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 48.6588 - val_loss: 7.0089\n",
            "Epoch 152/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.6729 - val_loss: 12.3761\n",
            "Epoch 153/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 47.0027 - val_loss: 7.2355\n",
            "Epoch 154/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 48.4207 - val_loss: 21.2182\n",
            "Epoch 155/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 31.8779 - val_loss: 11.5474\n",
            "Epoch 156/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.3858 - val_loss: 11.9476\n",
            "Epoch 157/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 47.6095 - val_loss: 11.0701\n",
            "Epoch 158/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 39.8599 - val_loss: 11.0090\n",
            "Epoch 159/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 34.2555 - val_loss: 9.4591\n",
            "Epoch 160/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 41.1836 - val_loss: 11.9615\n",
            "Epoch 161/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.4273 - val_loss: 7.1308\n",
            "Epoch 162/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 40.4897 - val_loss: 6.4567\n",
            "Epoch 163/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.2140 - val_loss: 12.7450\n",
            "Epoch 164/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 34.0796 - val_loss: 7.9180\n",
            "Epoch 165/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 36.6130 - val_loss: 12.8907\n",
            "Epoch 166/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 42.8836 - val_loss: 13.2507\n",
            "Epoch 167/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 37.1021 - val_loss: 6.9873\n",
            "Epoch 168/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 39.1658 - val_loss: 6.9517\n",
            "Epoch 169/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.4223 - val_loss: 13.3351\n",
            "Epoch 170/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 31.5613 - val_loss: 6.4224\n",
            "Epoch 171/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 36.5036 - val_loss: 8.3636\n",
            "Epoch 172/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 39.0020 - val_loss: 6.4077\n",
            "Epoch 173/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.0121 - val_loss: 7.5085\n",
            "Epoch 174/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 40.7764 - val_loss: 10.3202\n",
            "Epoch 175/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 31.2478 - val_loss: 6.8136\n",
            "Epoch 176/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 41.8066 - val_loss: 13.3598\n",
            "Epoch 177/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.3511 - val_loss: 7.6120\n",
            "Epoch 178/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 36.0702 - val_loss: 10.6575\n",
            "Epoch 179/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 45.5912 - val_loss: 8.0989\n",
            "Epoch 180/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 28.6110 - val_loss: 7.3932\n",
            "Epoch 181/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 31.6522 - val_loss: 8.9832\n",
            "Epoch 182/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.5397 - val_loss: 8.8725\n",
            "Epoch 183/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.3389 - val_loss: 11.0780\n",
            "Epoch 184/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.1092 - val_loss: 15.7325\n",
            "Epoch 185/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.9446 - val_loss: 12.2932\n",
            "Epoch 186/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.0229 - val_loss: 14.9923\n",
            "Epoch 187/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 34.0967 - val_loss: 6.0449\n",
            "Epoch 188/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.3679 - val_loss: 9.2321\n",
            "Epoch 189/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 36.0057 - val_loss: 8.7837\n",
            "Epoch 190/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 38.5937 - val_loss: 7.9870\n",
            "Epoch 191/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.6580 - val_loss: 6.9194\n",
            "Epoch 192/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.2424 - val_loss: 10.4086\n",
            "Epoch 193/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 33.2250 - val_loss: 13.2921\n",
            "Epoch 194/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 37.5031 - val_loss: 7.1246\n",
            "Epoch 195/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.5951 - val_loss: 18.3179\n",
            "Epoch 196/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.3148 - val_loss: 7.2848\n",
            "Epoch 197/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 41.2996 - val_loss: 9.1568\n",
            "Epoch 198/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 35.1302 - val_loss: 11.2687\n",
            "Epoch 199/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 36.3737 - val_loss: 6.5415\n",
            "Epoch 200/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 33.2871 - val_loss: 11.6572\n",
            "Epoch 201/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.3667 - val_loss: 9.9705\n",
            "Epoch 202/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.3283 - val_loss: 7.5090\n",
            "Epoch 203/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.8646 - val_loss: 6.6389\n",
            "Epoch 204/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 39.3787 - val_loss: 10.6484\n",
            "Epoch 205/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.9394 - val_loss: 7.8624\n",
            "Epoch 206/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 40.5162 - val_loss: 11.8568\n",
            "Epoch 207/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 34.3285 - val_loss: 11.9592\n",
            "Epoch 208/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 28.9698 - val_loss: 9.0037\n",
            "Epoch 209/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.8868 - val_loss: 12.2629\n",
            "Epoch 210/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 37.6255 - val_loss: 15.0664\n",
            "Epoch 211/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 37.3283 - val_loss: 11.0161\n",
            "Epoch 212/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.2038 - val_loss: 6.5409\n",
            "Epoch 213/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.1811 - val_loss: 7.9694\n",
            "Epoch 214/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.1029 - val_loss: 9.6429\n",
            "Epoch 215/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.5011 - val_loss: 10.3801\n",
            "Epoch 216/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 38.0034 - val_loss: 6.2193\n",
            "Epoch 217/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.0649 - val_loss: 10.7341\n",
            "Epoch 218/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.1088 - val_loss: 6.4357\n",
            "Epoch 219/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 33.3921 - val_loss: 9.9591\n",
            "Epoch 220/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 34.8690 - val_loss: 6.4562\n",
            "Epoch 221/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.8272 - val_loss: 6.0547\n",
            "Epoch 222/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.7007 - val_loss: 9.6363\n",
            "Epoch 223/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.7250 - val_loss: 9.0615\n",
            "Epoch 224/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.7977 - val_loss: 9.0515\n",
            "Epoch 225/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.3478 - val_loss: 5.1715\n",
            "Epoch 226/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 31.6609 - val_loss: 15.1964\n",
            "Epoch 227/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.2000 - val_loss: 7.8432\n",
            "Epoch 228/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.1576 - val_loss: 7.7200\n",
            "Epoch 229/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.4014 - val_loss: 8.0695\n",
            "Epoch 230/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.8350 - val_loss: 5.9493\n",
            "Epoch 231/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.6690 - val_loss: 7.3325\n",
            "Epoch 232/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.9845 - val_loss: 8.8332\n",
            "Epoch 233/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.8438 - val_loss: 6.1469\n",
            "Epoch 234/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.3275 - val_loss: 7.9000\n",
            "Epoch 235/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.7449 - val_loss: 6.2970\n",
            "Epoch 236/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.1636 - val_loss: 12.2650\n",
            "Epoch 237/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.0832 - val_loss: 5.6502\n",
            "Epoch 238/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.7803 - val_loss: 8.4116\n",
            "Epoch 239/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.1316 - val_loss: 6.1867\n",
            "Epoch 240/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.7370 - val_loss: 7.6305\n",
            "Epoch 241/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.2917 - val_loss: 10.6799\n",
            "Epoch 242/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.8446 - val_loss: 6.9602\n",
            "Epoch 243/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.4025 - val_loss: 6.0458\n",
            "Epoch 244/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 28.8993 - val_loss: 9.3175\n",
            "Epoch 245/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.3333 - val_loss: 5.0577\n",
            "Epoch 246/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.9903 - val_loss: 5.2556\n",
            "Epoch 247/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.1240 - val_loss: 7.4418\n",
            "Epoch 248/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.8207 - val_loss: 5.4699\n",
            "Epoch 249/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 33.2224 - val_loss: 7.6221\n",
            "Epoch 250/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.5297 - val_loss: 6.8679\n",
            "Epoch 251/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 31.6485 - val_loss: 8.1393\n",
            "Epoch 252/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.9103 - val_loss: 7.3930\n",
            "Epoch 253/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.2959 - val_loss: 7.7610\n",
            "Epoch 254/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.5874 - val_loss: 6.9767\n",
            "Epoch 255/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.4319 - val_loss: 8.9343\n",
            "Epoch 256/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.3403 - val_loss: 6.2466\n",
            "Epoch 257/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.1415 - val_loss: 6.1631\n",
            "Epoch 258/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.0488 - val_loss: 11.8616\n",
            "Epoch 259/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.9882 - val_loss: 6.5419\n",
            "Epoch 260/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.1694 - val_loss: 11.6975\n",
            "Epoch 261/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 31.3650 - val_loss: 7.5351\n",
            "Epoch 262/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.2716 - val_loss: 5.9628\n",
            "Epoch 263/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.0948 - val_loss: 7.2261\n",
            "Epoch 264/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.5850 - val_loss: 7.9381\n",
            "Epoch 265/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.3179 - val_loss: 9.4361\n",
            "Epoch 266/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 31.3888 - val_loss: 10.8345\n",
            "Epoch 267/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.8244 - val_loss: 6.6801\n",
            "Epoch 268/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.9787 - val_loss: 7.1992\n",
            "Epoch 269/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.2659 - val_loss: 7.1039\n",
            "Epoch 270/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 28.1435 - val_loss: 5.7168\n",
            "Epoch 271/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.6902 - val_loss: 5.6943\n",
            "Epoch 272/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.1626 - val_loss: 5.7444\n",
            "Epoch 273/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.2836 - val_loss: 7.9743\n",
            "Epoch 274/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 28.1717 - val_loss: 7.7400\n",
            "Epoch 275/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.6525 - val_loss: 5.4003\n",
            "Epoch 276/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.5761 - val_loss: 8.1331\n",
            "Epoch 277/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.4998 - val_loss: 6.8077\n",
            "Epoch 278/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.4556 - val_loss: 5.3913\n",
            "Epoch 279/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.6855 - val_loss: 8.2473\n",
            "Epoch 280/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.5902 - val_loss: 5.6644\n",
            "Epoch 281/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.7841 - val_loss: 5.8803\n",
            "Epoch 282/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.9653 - val_loss: 6.0734\n",
            "Epoch 283/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.0532 - val_loss: 7.1036\n",
            "Epoch 284/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.0633 - val_loss: 5.6332\n",
            "Epoch 285/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.3997 - val_loss: 5.7228\n",
            "Epoch 286/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.1281 - val_loss: 6.3999\n",
            "Epoch 287/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.2064 - val_loss: 7.8026\n",
            "Epoch 288/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.6971 - val_loss: 6.5290\n",
            "Epoch 289/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.1594 - val_loss: 6.0606\n",
            "Epoch 290/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.9909 - val_loss: 5.6878\n",
            "Epoch 291/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.4302 - val_loss: 7.2936\n",
            "Epoch 292/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.4327 - val_loss: 7.2884\n",
            "Epoch 293/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 35.1502 - val_loss: 7.2020\n",
            "Epoch 294/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.8373 - val_loss: 6.0085\n",
            "Epoch 295/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 28.4488 - val_loss: 7.5227\n",
            "Epoch 296/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.9613 - val_loss: 5.8177\n",
            "Epoch 297/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.4308 - val_loss: 6.1821\n",
            "Epoch 298/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.9535 - val_loss: 6.3464\n",
            "Epoch 299/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.2157 - val_loss: 7.0321\n",
            "Epoch 300/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.4658 - val_loss: 10.3982\n",
            "Epoch 301/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.5582 - val_loss: 5.2122\n",
            "Epoch 302/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.0306 - val_loss: 5.2945\n",
            "Epoch 303/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.3232 - val_loss: 12.6471\n",
            "Epoch 304/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 34.0612 - val_loss: 6.7589\n",
            "Epoch 305/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.5800 - val_loss: 7.7113\n",
            "Epoch 306/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.2042 - val_loss: 5.1648\n",
            "Epoch 307/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.6367 - val_loss: 6.0739\n",
            "Epoch 308/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.5726 - val_loss: 5.7411\n",
            "Epoch 309/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 28.7686 - val_loss: 6.8103\n",
            "Epoch 310/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.2320 - val_loss: 5.6494\n",
            "Epoch 311/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.3892 - val_loss: 8.0285\n",
            "Epoch 312/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.1812 - val_loss: 7.6043\n",
            "Epoch 313/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.6850 - val_loss: 7.1893\n",
            "Epoch 314/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.5135 - val_loss: 5.5094\n",
            "Epoch 315/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.7669 - val_loss: 7.6409\n",
            "Epoch 316/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.9051 - val_loss: 5.4752\n",
            "Epoch 317/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.1232 - val_loss: 5.4373\n",
            "Epoch 318/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.2593 - val_loss: 5.5400\n",
            "Epoch 319/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.3446 - val_loss: 5.4278\n",
            "Epoch 320/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.7454 - val_loss: 6.5278\n",
            "Epoch 321/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.1119 - val_loss: 5.0663\n",
            "Epoch 322/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.0758 - val_loss: 6.0991\n",
            "Epoch 323/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.7462 - val_loss: 9.1722\n",
            "Epoch 324/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.6412 - val_loss: 6.0710\n",
            "Epoch 325/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.7331 - val_loss: 6.2189\n",
            "Epoch 326/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.9353 - val_loss: 7.5623\n",
            "Epoch 327/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.3384 - val_loss: 5.1825\n",
            "Epoch 328/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.9755 - val_loss: 5.2478\n",
            "Epoch 329/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5254 - val_loss: 6.4538\n",
            "Epoch 330/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.8668 - val_loss: 8.0806\n",
            "Epoch 331/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.0648 - val_loss: 7.7866\n",
            "Epoch 332/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.7890 - val_loss: 4.9429\n",
            "Epoch 333/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.0429 - val_loss: 11.0284\n",
            "Epoch 334/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.2663 - val_loss: 4.7698\n",
            "Epoch 335/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 28.0284 - val_loss: 8.6876\n",
            "Epoch 336/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.7316 - val_loss: 4.6912\n",
            "Epoch 337/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.9701 - val_loss: 5.6956\n",
            "Epoch 338/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.2926 - val_loss: 5.0389\n",
            "Epoch 339/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.6576 - val_loss: 5.5416\n",
            "Epoch 340/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.0169 - val_loss: 6.4245\n",
            "Epoch 341/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.4855 - val_loss: 5.6095\n",
            "Epoch 342/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.0128 - val_loss: 5.7411\n",
            "Epoch 343/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.6531 - val_loss: 5.7836\n",
            "Epoch 344/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.6216 - val_loss: 7.7400\n",
            "Epoch 345/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.8061 - val_loss: 5.1299\n",
            "Epoch 346/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 32.3589 - val_loss: 4.8092\n",
            "Epoch 347/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.5639 - val_loss: 7.0238\n",
            "Epoch 348/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.9069 - val_loss: 4.8726\n",
            "Epoch 349/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.3420 - val_loss: 8.2232\n",
            "Epoch 350/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.9269 - val_loss: 4.7084\n",
            "Epoch 351/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 29.8837 - val_loss: 5.1489\n",
            "Epoch 352/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.3297 - val_loss: 8.0008\n",
            "Epoch 353/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.0121 - val_loss: 7.1454\n",
            "Epoch 354/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.8506 - val_loss: 6.4773\n",
            "Epoch 355/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.8697 - val_loss: 5.2433\n",
            "Epoch 356/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.4880 - val_loss: 5.9121\n",
            "Epoch 357/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.5706 - val_loss: 5.1275\n",
            "Epoch 358/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.4164 - val_loss: 7.2643\n",
            "Epoch 359/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.7675 - val_loss: 4.8556\n",
            "Epoch 360/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.6306 - val_loss: 4.8457\n",
            "Epoch 361/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.7353 - val_loss: 6.6318\n",
            "Epoch 362/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.9603 - val_loss: 4.1102\n",
            "Epoch 363/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.8619 - val_loss: 6.9477\n",
            "Epoch 364/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.9013 - val_loss: 5.2064\n",
            "Epoch 365/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.0513 - val_loss: 6.5718\n",
            "Epoch 366/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.1627 - val_loss: 7.2860\n",
            "Epoch 367/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.2692 - val_loss: 6.2477\n",
            "Epoch 368/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.7462 - val_loss: 5.4659\n",
            "Epoch 369/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.4041 - val_loss: 5.5575\n",
            "Epoch 370/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.3935 - val_loss: 5.1980\n",
            "Epoch 371/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.1503 - val_loss: 5.7663\n",
            "Epoch 372/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.8235 - val_loss: 8.7475\n",
            "Epoch 373/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 28.9896 - val_loss: 4.9876\n",
            "Epoch 374/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.8152 - val_loss: 7.1113\n",
            "Epoch 375/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.1036 - val_loss: 6.1477\n",
            "Epoch 376/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.3445 - val_loss: 5.5798\n",
            "Epoch 377/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.0112 - val_loss: 4.9463\n",
            "Epoch 378/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.3496 - val_loss: 5.6342\n",
            "Epoch 379/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.3334 - val_loss: 14.2872\n",
            "Epoch 380/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.8014 - val_loss: 4.9928\n",
            "Epoch 381/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.9679 - val_loss: 5.4025\n",
            "Epoch 382/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.1079 - val_loss: 8.0818\n",
            "Epoch 383/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.4894 - val_loss: 6.3342\n",
            "Epoch 384/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7691 - val_loss: 5.2583\n",
            "Epoch 385/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.3895 - val_loss: 7.5472\n",
            "Epoch 386/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7148 - val_loss: 5.0700\n",
            "Epoch 387/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.6930 - val_loss: 11.0320\n",
            "Epoch 388/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 30.5179 - val_loss: 4.5228\n",
            "Epoch 389/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.3508 - val_loss: 6.0340\n",
            "Epoch 390/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.5195 - val_loss: 5.8494\n",
            "Epoch 391/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.4209 - val_loss: 8.3552\n",
            "Epoch 392/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.9453 - val_loss: 4.7796\n",
            "Epoch 393/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.8394 - val_loss: 5.1920\n",
            "Epoch 394/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.5594 - val_loss: 5.9110\n",
            "Epoch 395/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.4362 - val_loss: 5.2617\n",
            "Epoch 396/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.6734 - val_loss: 7.8408\n",
            "Epoch 397/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.0937 - val_loss: 5.0291\n",
            "Epoch 398/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.3929 - val_loss: 4.7571\n",
            "Epoch 399/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.6550 - val_loss: 6.3184\n",
            "Epoch 400/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.7753 - val_loss: 5.8134\n",
            "Epoch 401/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.2652 - val_loss: 4.1415\n",
            "Epoch 402/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.3273 - val_loss: 4.9362\n",
            "Epoch 403/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.8178 - val_loss: 7.7914\n",
            "Epoch 404/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.0269 - val_loss: 6.2934\n",
            "Epoch 405/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.6970 - val_loss: 6.0847\n",
            "Epoch 406/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.9526 - val_loss: 6.1241\n",
            "Epoch 407/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.9210 - val_loss: 4.8081\n",
            "Epoch 408/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.7728 - val_loss: 8.0491\n",
            "Epoch 409/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.2430 - val_loss: 6.0178\n",
            "Epoch 410/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.7632 - val_loss: 4.5798\n",
            "Epoch 411/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.4371 - val_loss: 4.9125\n",
            "Epoch 412/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.6981 - val_loss: 5.0262\n",
            "Epoch 413/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.9345 - val_loss: 5.4545\n",
            "Epoch 414/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.8933 - val_loss: 4.7076\n",
            "Epoch 415/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.8654 - val_loss: 5.8057\n",
            "Epoch 416/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.6588 - val_loss: 5.6584\n",
            "Epoch 417/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.9728 - val_loss: 5.3669\n",
            "Epoch 418/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.1821 - val_loss: 5.2204\n",
            "Epoch 419/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.3377 - val_loss: 6.6962\n",
            "Epoch 420/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5626 - val_loss: 7.5661\n",
            "Epoch 421/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.8488 - val_loss: 4.6898\n",
            "Epoch 422/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1790 - val_loss: 5.7008\n",
            "Epoch 423/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.6128 - val_loss: 5.0992\n",
            "Epoch 424/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.6996 - val_loss: 6.7982\n",
            "Epoch 425/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 25.1650 - val_loss: 5.0823\n",
            "Epoch 426/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7133 - val_loss: 4.6995\n",
            "Epoch 427/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.5636 - val_loss: 4.6565\n",
            "Epoch 428/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.2098 - val_loss: 4.9540\n",
            "Epoch 429/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3773 - val_loss: 4.6806\n",
            "Epoch 430/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.8365 - val_loss: 5.9099\n",
            "Epoch 431/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.0763 - val_loss: 5.5371\n",
            "Epoch 432/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.9035 - val_loss: 5.2003\n",
            "Epoch 433/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.3171 - val_loss: 5.3469\n",
            "Epoch 434/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 26.3882 - val_loss: 4.7921\n",
            "Epoch 435/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.0003 - val_loss: 5.3406\n",
            "Epoch 436/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.7600 - val_loss: 5.3630\n",
            "Epoch 437/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.6085 - val_loss: 4.9571\n",
            "Epoch 438/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.9117 - val_loss: 5.8030\n",
            "Epoch 439/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.0990 - val_loss: 4.8576\n",
            "Epoch 440/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.9592 - val_loss: 4.6456\n",
            "Epoch 441/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.9858 - val_loss: 7.7643\n",
            "Epoch 442/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.1201 - val_loss: 6.0181\n",
            "Epoch 443/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.7475 - val_loss: 4.6089\n",
            "Epoch 444/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.6331 - val_loss: 5.8331\n",
            "Epoch 445/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.3883 - val_loss: 5.2478\n",
            "Epoch 446/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.0806 - val_loss: 7.6132\n",
            "Epoch 447/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.3644 - val_loss: 4.6362\n",
            "Epoch 448/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.7457 - val_loss: 9.0193\n",
            "Epoch 449/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.7568 - val_loss: 4.1567\n",
            "Epoch 450/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.0543 - val_loss: 4.4729\n",
            "Epoch 451/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.9323 - val_loss: 4.7152\n",
            "Epoch 452/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.9560 - val_loss: 5.4579\n",
            "Epoch 453/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.4386 - val_loss: 6.0984\n",
            "Epoch 454/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.2677 - val_loss: 6.2753\n",
            "Epoch 455/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.7331 - val_loss: 4.2557\n",
            "Epoch 456/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.0132 - val_loss: 5.7543\n",
            "Epoch 457/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.7343 - val_loss: 4.5120\n",
            "Epoch 458/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.4082 - val_loss: 4.6177\n",
            "Epoch 459/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.3760 - val_loss: 5.5741\n",
            "Epoch 460/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2888 - val_loss: 5.4582\n",
            "Epoch 461/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.8318 - val_loss: 4.6097\n",
            "Epoch 462/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.1260 - val_loss: 5.0921\n",
            "Epoch 463/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.8782 - val_loss: 5.3676\n",
            "Epoch 464/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.1625 - val_loss: 4.4291\n",
            "Epoch 465/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1894 - val_loss: 4.2969\n",
            "Epoch 466/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.1111 - val_loss: 5.8953\n",
            "Epoch 467/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.1012 - val_loss: 5.0079\n",
            "Epoch 468/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.1606 - val_loss: 4.9452\n",
            "Epoch 469/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.1381 - val_loss: 4.6242\n",
            "Epoch 470/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.5929 - val_loss: 4.3627\n",
            "Epoch 471/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.1238 - val_loss: 4.5496\n",
            "Epoch 472/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.3623 - val_loss: 4.8965\n",
            "Epoch 473/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.9029 - val_loss: 4.9117\n",
            "Epoch 474/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.0236 - val_loss: 4.4286\n",
            "Epoch 475/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.5104 - val_loss: 4.4734\n",
            "Epoch 476/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.1928 - val_loss: 4.0777\n",
            "Epoch 477/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.6571 - val_loss: 3.8632\n",
            "Epoch 478/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.3951 - val_loss: 3.8568\n",
            "Epoch 479/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.9953 - val_loss: 4.0737\n",
            "Epoch 480/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.2047 - val_loss: 3.9177\n",
            "Epoch 481/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.9357 - val_loss: 3.9789\n",
            "Epoch 482/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.3494 - val_loss: 4.4524\n",
            "Epoch 483/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.0449 - val_loss: 4.7037\n",
            "Epoch 484/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.4941 - val_loss: 3.9277\n",
            "Epoch 485/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.6999 - val_loss: 4.2531\n",
            "Epoch 486/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.1988 - val_loss: 7.0091\n",
            "Epoch 487/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.9304 - val_loss: 4.4383\n",
            "Epoch 488/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5499 - val_loss: 5.6784\n",
            "Epoch 489/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3115 - val_loss: 4.4815\n",
            "Epoch 490/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.2653 - val_loss: 4.6052\n",
            "Epoch 491/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.2902 - val_loss: 4.4999\n",
            "Epoch 492/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7554 - val_loss: 4.1243\n",
            "Epoch 493/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.7326 - val_loss: 4.0271\n",
            "Epoch 494/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.8949 - val_loss: 3.9344\n",
            "Epoch 495/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.2072 - val_loss: 4.1436\n",
            "Epoch 496/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.3449 - val_loss: 3.5913\n",
            "Epoch 497/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.3500 - val_loss: 4.6257\n",
            "Epoch 498/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.6276 - val_loss: 4.9894\n",
            "Epoch 499/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5718 - val_loss: 5.3368\n",
            "Epoch 500/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.9110 - val_loss: 5.1045\n",
            "Epoch 501/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5874 - val_loss: 6.8963\n",
            "Epoch 502/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.5448 - val_loss: 4.8471\n",
            "Epoch 503/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.4175 - val_loss: 5.1276\n",
            "Epoch 504/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.6423 - val_loss: 5.9680\n",
            "Epoch 505/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.2891 - val_loss: 5.6310\n",
            "Epoch 506/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.2548 - val_loss: 4.4817\n",
            "Epoch 507/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.9333 - val_loss: 5.0124\n",
            "Epoch 508/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.3910 - val_loss: 4.4928\n",
            "Epoch 509/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.0560 - val_loss: 4.8310\n",
            "Epoch 510/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.9482 - val_loss: 4.2688\n",
            "Epoch 511/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.5613 - val_loss: 3.9220\n",
            "Epoch 512/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.9756 - val_loss: 4.2817\n",
            "Epoch 513/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.4995 - val_loss: 4.3625\n",
            "Epoch 514/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.2872 - val_loss: 4.6401\n",
            "Epoch 515/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.6312 - val_loss: 4.0137\n",
            "Epoch 516/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 24.2580 - val_loss: 5.0387\n",
            "Epoch 517/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.2276 - val_loss: 4.0949\n",
            "Epoch 518/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5200 - val_loss: 3.9748\n",
            "Epoch 519/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.1531 - val_loss: 4.0443\n",
            "Epoch 520/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.9590 - val_loss: 4.0131\n",
            "Epoch 521/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1077 - val_loss: 5.1206\n",
            "Epoch 522/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.2691 - val_loss: 4.0452\n",
            "Epoch 523/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1799 - val_loss: 3.9635\n",
            "Epoch 524/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.2471 - val_loss: 4.1023\n",
            "Epoch 525/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2275 - val_loss: 3.9552\n",
            "Epoch 526/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.7124 - val_loss: 4.1805\n",
            "Epoch 527/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.8092 - val_loss: 4.0637\n",
            "Epoch 528/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.4576 - val_loss: 4.7279\n",
            "Epoch 529/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.0292 - val_loss: 4.6442\n",
            "Epoch 530/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.9876 - val_loss: 5.2882\n",
            "Epoch 531/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.2787 - val_loss: 5.3583\n",
            "Epoch 532/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.8900 - val_loss: 4.1686\n",
            "Epoch 533/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.9632 - val_loss: 4.5138\n",
            "Epoch 534/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.6498 - val_loss: 5.0750\n",
            "Epoch 535/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.6950 - val_loss: 4.3670\n",
            "Epoch 536/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.1483 - val_loss: 5.6923\n",
            "Epoch 537/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.3459 - val_loss: 4.0302\n",
            "Epoch 538/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.1444 - val_loss: 3.5474\n",
            "Epoch 539/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.0169 - val_loss: 4.1269\n",
            "Epoch 540/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1609 - val_loss: 3.8226\n",
            "Epoch 541/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.4577 - val_loss: 4.8095\n",
            "Epoch 542/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.9626 - val_loss: 5.4910\n",
            "Epoch 543/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.9950 - val_loss: 4.1821\n",
            "Epoch 544/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.4463 - val_loss: 5.5332\n",
            "Epoch 545/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.7095 - val_loss: 4.4492\n",
            "Epoch 546/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.7300 - val_loss: 5.4048\n",
            "Epoch 547/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.1820 - val_loss: 4.2934\n",
            "Epoch 548/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.4186 - val_loss: 3.7234\n",
            "Epoch 549/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.1783 - val_loss: 3.7918\n",
            "Epoch 550/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.8588 - val_loss: 5.6734\n",
            "Epoch 551/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.6816 - val_loss: 4.1717\n",
            "Epoch 552/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7849 - val_loss: 4.8005\n",
            "Epoch 553/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5548 - val_loss: 4.4520\n",
            "Epoch 554/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.9084 - val_loss: 5.0922\n",
            "Epoch 555/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3850 - val_loss: 3.7731\n",
            "Epoch 556/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.4589 - val_loss: 3.6682\n",
            "Epoch 557/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.5922 - val_loss: 4.4914\n",
            "Epoch 558/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.7221 - val_loss: 3.3417\n",
            "Epoch 559/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.9480 - val_loss: 3.0610\n",
            "Epoch 560/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.6674 - val_loss: 2.8181\n",
            "Epoch 561/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.3629 - val_loss: 3.2675\n",
            "Epoch 562/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.4606 - val_loss: 3.2191\n",
            "Epoch 563/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2894 - val_loss: 3.7166\n",
            "Epoch 564/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.7130 - val_loss: 3.2877\n",
            "Epoch 565/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.3948 - val_loss: 3.1254\n",
            "Epoch 566/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.8882 - val_loss: 3.5245\n",
            "Epoch 567/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.1893 - val_loss: 4.1226\n",
            "Epoch 568/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.1357 - val_loss: 4.3688\n",
            "Epoch 569/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.2681 - val_loss: 4.3028\n",
            "Epoch 570/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.0115 - val_loss: 3.3758\n",
            "Epoch 571/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.4669 - val_loss: 5.1977\n",
            "Epoch 572/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 27.2641 - val_loss: 3.6045\n",
            "Epoch 573/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.7972 - val_loss: 3.4108\n",
            "Epoch 574/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.7743 - val_loss: 3.5308\n",
            "Epoch 575/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.9174 - val_loss: 3.4287\n",
            "Epoch 576/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.7313 - val_loss: 3.8291\n",
            "Epoch 577/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.8040 - val_loss: 5.3666\n",
            "Epoch 578/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.1489 - val_loss: 3.6052\n",
            "Epoch 579/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.9175 - val_loss: 4.1573\n",
            "Epoch 580/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.7816 - val_loss: 4.3127\n",
            "Epoch 581/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.5439 - val_loss: 4.6756\n",
            "Epoch 582/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.3549 - val_loss: 3.5026\n",
            "Epoch 583/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1750 - val_loss: 3.2146\n",
            "Epoch 584/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.4645 - val_loss: 3.1343\n",
            "Epoch 585/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.1732 - val_loss: 3.1567\n",
            "Epoch 586/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.0350 - val_loss: 4.5257\n",
            "Epoch 587/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.3895 - val_loss: 3.4770\n",
            "Epoch 588/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.4510 - val_loss: 3.6635\n",
            "Epoch 589/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.6693 - val_loss: 3.6511\n",
            "Epoch 590/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.0624 - val_loss: 4.3966\n",
            "Epoch 591/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.4124 - val_loss: 4.0232\n",
            "Epoch 592/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.7382 - val_loss: 4.3236\n",
            "Epoch 593/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.1189 - val_loss: 5.0290\n",
            "Epoch 594/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5524 - val_loss: 4.9971\n",
            "Epoch 595/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.0483 - val_loss: 4.9590\n",
            "Epoch 596/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.3901 - val_loss: 3.8878\n",
            "Epoch 597/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7626 - val_loss: 3.6119\n",
            "Epoch 598/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.0249 - val_loss: 4.0123\n",
            "Epoch 599/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1167 - val_loss: 3.9754\n",
            "Epoch 600/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.7721 - val_loss: 3.9011\n",
            "Epoch 601/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.6747 - val_loss: 4.9591\n",
            "Epoch 602/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.8047 - val_loss: 4.0851\n",
            "Epoch 603/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.6327 - val_loss: 4.9616\n",
            "Epoch 604/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.9181 - val_loss: 3.6984\n",
            "Epoch 605/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.5578 - val_loss: 4.8830\n",
            "Epoch 606/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5764 - val_loss: 5.4801\n",
            "Epoch 607/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.1388 - val_loss: 3.8350\n",
            "Epoch 608/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.9599 - val_loss: 4.0779\n",
            "Epoch 609/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.6094 - val_loss: 4.0916\n",
            "Epoch 610/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.8635 - val_loss: 3.7692\n",
            "Epoch 611/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.0964 - val_loss: 5.2167\n",
            "Epoch 612/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.6826 - val_loss: 4.0979\n",
            "Epoch 613/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.4225 - val_loss: 8.1458\n",
            "Epoch 614/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1893 - val_loss: 3.7415\n",
            "Epoch 615/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.5304 - val_loss: 3.1637\n",
            "Epoch 616/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.6910 - val_loss: 3.3487\n",
            "Epoch 617/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5493 - val_loss: 5.9438\n",
            "Epoch 618/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.7412 - val_loss: 5.1632\n",
            "Epoch 619/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.5187 - val_loss: 3.5708\n",
            "Epoch 620/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.7252 - val_loss: 3.0378\n",
            "Epoch 621/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.9566 - val_loss: 3.8102\n",
            "Epoch 622/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.9506 - val_loss: 2.9351\n",
            "Epoch 623/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.1463 - val_loss: 3.2622\n",
            "Epoch 624/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.0666 - val_loss: 5.5431\n",
            "Epoch 625/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.4095 - val_loss: 6.1358\n",
            "Epoch 626/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.8868 - val_loss: 5.2004\n",
            "Epoch 627/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.2251 - val_loss: 8.4238\n",
            "Epoch 628/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.4097 - val_loss: 3.4722\n",
            "Epoch 629/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.1373 - val_loss: 4.9781\n",
            "Epoch 630/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.6157 - val_loss: 4.1872\n",
            "Epoch 631/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.9958 - val_loss: 4.1414\n",
            "Epoch 632/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.1973 - val_loss: 5.9642\n",
            "Epoch 633/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7011 - val_loss: 4.2451\n",
            "Epoch 634/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.9310 - val_loss: 4.0381\n",
            "Epoch 635/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.9597 - val_loss: 4.4465\n",
            "Epoch 636/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.8570 - val_loss: 6.8921\n",
            "Epoch 637/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.8613 - val_loss: 5.5034\n",
            "Epoch 638/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.1631 - val_loss: 3.6375\n",
            "Epoch 639/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5461 - val_loss: 4.0673\n",
            "Epoch 640/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.9740 - val_loss: 5.7013\n",
            "Epoch 641/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.2786 - val_loss: 4.9525\n",
            "Epoch 642/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.8909 - val_loss: 3.7481\n",
            "Epoch 643/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1291 - val_loss: 4.3023\n",
            "Epoch 644/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.4464 - val_loss: 4.9101\n",
            "Epoch 645/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.7737 - val_loss: 4.8336\n",
            "Epoch 646/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.7260 - val_loss: 4.5569\n",
            "Epoch 647/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.6594 - val_loss: 7.4591\n",
            "Epoch 648/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 23.6645 - val_loss: 4.0176\n",
            "Epoch 649/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.2516 - val_loss: 3.7611\n",
            "Epoch 650/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.8992 - val_loss: 3.4421\n",
            "Epoch 651/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.6962 - val_loss: 3.1549\n",
            "Epoch 652/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.7127 - val_loss: 2.8758\n",
            "Epoch 653/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1460 - val_loss: 4.0020\n",
            "Epoch 654/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.6659 - val_loss: 5.6646\n",
            "Epoch 655/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.4167 - val_loss: 5.3456\n",
            "Epoch 656/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.1319 - val_loss: 3.7577\n",
            "Epoch 657/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.8317 - val_loss: 5.8487\n",
            "Epoch 658/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.7571 - val_loss: 4.4601\n",
            "Epoch 659/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.5703 - val_loss: 4.8152\n",
            "Epoch 660/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.6388 - val_loss: 3.9909\n",
            "Epoch 661/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.4121 - val_loss: 4.4700\n",
            "Epoch 662/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.6891 - val_loss: 5.0030\n",
            "Epoch 663/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.8678 - val_loss: 4.6810\n",
            "Epoch 664/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.0469 - val_loss: 6.8359\n",
            "Epoch 665/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.4545 - val_loss: 4.6393\n",
            "Epoch 666/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.6307 - val_loss: 4.2649\n",
            "Epoch 667/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.5466 - val_loss: 4.8527\n",
            "Epoch 668/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1812 - val_loss: 4.0808\n",
            "Epoch 669/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.7507 - val_loss: 4.9058\n",
            "Epoch 670/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.2079 - val_loss: 8.8668\n",
            "Epoch 671/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.7118 - val_loss: 3.4494\n",
            "Epoch 672/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.9567 - val_loss: 3.2081\n",
            "Epoch 673/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.7934 - val_loss: 4.1431\n",
            "Epoch 674/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1194 - val_loss: 2.8251\n",
            "Epoch 675/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.0057 - val_loss: 4.7751\n",
            "Epoch 676/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.3985 - val_loss: 4.4501\n",
            "Epoch 677/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.9788 - val_loss: 4.1231\n",
            "Epoch 678/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.3451 - val_loss: 3.6159\n",
            "Epoch 679/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.6812 - val_loss: 4.7803\n",
            "Epoch 680/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.3471 - val_loss: 4.8807\n",
            "Epoch 681/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.8425 - val_loss: 3.2321\n",
            "Epoch 682/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.6374 - val_loss: 6.3577\n",
            "Epoch 683/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.4127 - val_loss: 4.9782\n",
            "Epoch 684/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.6104 - val_loss: 2.4059\n",
            "Epoch 685/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.6438 - val_loss: 4.2724\n",
            "Epoch 686/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.6496 - val_loss: 5.2576\n",
            "Epoch 687/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.7530 - val_loss: 4.6739\n",
            "Epoch 688/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.6072 - val_loss: 6.0230\n",
            "Epoch 689/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.9525 - val_loss: 3.5270\n",
            "Epoch 690/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5557 - val_loss: 4.1061\n",
            "Epoch 691/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.6934 - val_loss: 4.2644\n",
            "Epoch 692/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.0712 - val_loss: 3.1738\n",
            "Epoch 693/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.7359 - val_loss: 4.4546\n",
            "Epoch 694/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.2548 - val_loss: 8.6541\n",
            "Epoch 695/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.0353 - val_loss: 9.4848\n",
            "Epoch 696/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.2936 - val_loss: 5.5349\n",
            "Epoch 697/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.2442 - val_loss: 4.6336\n",
            "Epoch 698/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.1992 - val_loss: 3.8457\n",
            "Epoch 699/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3700 - val_loss: 5.6355\n",
            "Epoch 700/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.5673 - val_loss: 3.7969\n",
            "Epoch 701/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.0556 - val_loss: 4.8637\n",
            "Epoch 702/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.2213 - val_loss: 3.3362\n",
            "Epoch 703/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.4278 - val_loss: 7.3105\n",
            "Epoch 704/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5596 - val_loss: 3.6289\n",
            "Epoch 705/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.6455 - val_loss: 4.1556\n",
            "Epoch 706/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5129 - val_loss: 5.4033\n",
            "Epoch 707/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.4113 - val_loss: 4.7095\n",
            "Epoch 708/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.0591 - val_loss: 2.7815\n",
            "Epoch 709/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1142 - val_loss: 4.0597\n",
            "Epoch 710/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.0106 - val_loss: 3.9602\n",
            "Epoch 711/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1448 - val_loss: 3.1858\n",
            "Epoch 712/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.2179 - val_loss: 3.1698\n",
            "Epoch 713/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.2388 - val_loss: 5.0765\n",
            "Epoch 714/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.8898 - val_loss: 2.9321\n",
            "Epoch 715/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.5319 - val_loss: 2.6668\n",
            "Epoch 716/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.6322 - val_loss: 4.0659\n",
            "Epoch 717/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.1779 - val_loss: 4.0008\n",
            "Epoch 718/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.5233 - val_loss: 3.0407\n",
            "Epoch 719/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.3230 - val_loss: 3.9534\n",
            "Epoch 720/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5539 - val_loss: 3.4956\n",
            "Epoch 721/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.2347 - val_loss: 5.6271\n",
            "Epoch 722/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.7794 - val_loss: 4.2848\n",
            "Epoch 723/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.6295 - val_loss: 3.9340\n",
            "Epoch 724/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.5871 - val_loss: 5.3274\n",
            "Epoch 725/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.1832 - val_loss: 4.6465\n",
            "Epoch 726/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1169 - val_loss: 4.8902\n",
            "Epoch 727/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.7808 - val_loss: 4.3064\n",
            "Epoch 728/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.5484 - val_loss: 6.1773\n",
            "Epoch 729/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.8462 - val_loss: 3.3361\n",
            "Epoch 730/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.9028 - val_loss: 3.3917\n",
            "Epoch 731/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.6616 - val_loss: 3.5915\n",
            "Epoch 732/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.8865 - val_loss: 5.4657\n",
            "Epoch 733/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1596 - val_loss: 5.7847\n",
            "Epoch 734/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.6161 - val_loss: 5.6477\n",
            "Epoch 735/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.6460 - val_loss: 3.5210\n",
            "Epoch 736/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.9284 - val_loss: 4.8622\n",
            "Epoch 737/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.2275 - val_loss: 5.4863\n",
            "Epoch 738/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.1633 - val_loss: 3.6956\n",
            "Epoch 739/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.2364 - val_loss: 5.7144\n",
            "Epoch 740/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.2016 - val_loss: 4.8926\n",
            "Epoch 741/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.9112 - val_loss: 5.2859\n",
            "Epoch 742/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.3431 - val_loss: 5.3017\n",
            "Epoch 743/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2065 - val_loss: 5.9901\n",
            "Epoch 744/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.6884 - val_loss: 5.4791\n",
            "Epoch 745/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 9.4057 - val_loss: 4.8314\n",
            "Epoch 746/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.4575 - val_loss: 5.2859\n",
            "Epoch 747/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.3124 - val_loss: 5.5949\n",
            "Epoch 748/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.4996 - val_loss: 4.8123\n",
            "Epoch 749/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.5331 - val_loss: 4.4071\n",
            "Epoch 750/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.9964 - val_loss: 4.0000\n",
            "Epoch 751/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.5294 - val_loss: 3.6442\n",
            "Epoch 752/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.0113 - val_loss: 7.9847\n",
            "Epoch 753/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.4592 - val_loss: 4.8213\n",
            "Epoch 754/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.1878 - val_loss: 3.8799\n",
            "Epoch 755/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2240 - val_loss: 4.1404\n",
            "Epoch 756/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.2357 - val_loss: 4.4603\n",
            "Epoch 757/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 8.0121 - val_loss: 5.3046\n",
            "Epoch 758/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1685 - val_loss: 5.9567\n",
            "Epoch 759/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.2724 - val_loss: 7.0832\n",
            "Epoch 760/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 22.3971 - val_loss: 5.1986\n",
            "Epoch 761/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.2245 - val_loss: 5.1150\n",
            "Epoch 762/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.7869 - val_loss: 3.2665\n",
            "Epoch 763/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.8505 - val_loss: 4.8890\n",
            "Epoch 764/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.0698 - val_loss: 5.6834\n",
            "Epoch 765/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.2779 - val_loss: 3.7144\n",
            "Epoch 766/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1930 - val_loss: 7.3567\n",
            "Epoch 767/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.5019 - val_loss: 6.2065\n",
            "Epoch 768/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.3300 - val_loss: 5.1933\n",
            "Epoch 769/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.6201 - val_loss: 4.0749\n",
            "Epoch 770/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.5119 - val_loss: 9.8958\n",
            "Epoch 771/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.5933 - val_loss: 3.4261\n",
            "Epoch 772/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.6617 - val_loss: 3.8969\n",
            "Epoch 773/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1883 - val_loss: 7.5467\n",
            "Epoch 774/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.8584 - val_loss: 4.9384\n",
            "Epoch 775/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.4131 - val_loss: 4.1015\n",
            "Epoch 776/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.4156 - val_loss: 7.2295\n",
            "Epoch 777/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.4110 - val_loss: 4.5798\n",
            "Epoch 778/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.0512 - val_loss: 5.0727\n",
            "Epoch 779/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.4492 - val_loss: 5.3676\n",
            "Epoch 780/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.0966 - val_loss: 7.1740\n",
            "Epoch 781/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.4102 - val_loss: 3.2358\n",
            "Epoch 782/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.6806 - val_loss: 4.2427\n",
            "Epoch 783/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.3207 - val_loss: 3.4720\n",
            "Epoch 784/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.8031 - val_loss: 3.2952\n",
            "Epoch 785/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.9528 - val_loss: 5.1479\n",
            "Epoch 786/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2255 - val_loss: 3.8992\n",
            "Epoch 787/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 9.7069 - val_loss: 5.3006\n",
            "Epoch 788/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.0931 - val_loss: 3.9731\n",
            "Epoch 789/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7837 - val_loss: 3.7627\n",
            "Epoch 790/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 20.9958 - val_loss: 3.7262\n",
            "Epoch 791/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.8921 - val_loss: 5.9174\n",
            "Epoch 792/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.9972 - val_loss: 2.8357\n",
            "Epoch 793/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.5009 - val_loss: 4.6116\n",
            "Epoch 794/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.5421 - val_loss: 7.9298\n",
            "Epoch 795/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.2840 - val_loss: 4.2443\n",
            "Epoch 796/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.0936 - val_loss: 5.4609\n",
            "Epoch 797/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.9375 - val_loss: 7.5457\n",
            "Epoch 798/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1247 - val_loss: 4.2518\n",
            "Epoch 799/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.0305 - val_loss: 4.1466\n",
            "Epoch 800/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.2201 - val_loss: 5.3567\n",
            "Epoch 801/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3737 - val_loss: 4.3912\n",
            "Epoch 802/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 9.6019 - val_loss: 4.6295\n",
            "Epoch 803/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5729 - val_loss: 4.1927\n",
            "Epoch 804/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.6729 - val_loss: 4.5106\n",
            "Epoch 805/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.1887 - val_loss: 6.6346\n",
            "Epoch 806/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.0069 - val_loss: 3.8786\n",
            "Epoch 807/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1107 - val_loss: 4.8255\n",
            "Epoch 808/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1863 - val_loss: 4.6432\n",
            "Epoch 809/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.0682 - val_loss: 5.1941\n",
            "Epoch 810/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.9907 - val_loss: 5.2194\n",
            "Epoch 811/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 9.8985 - val_loss: 3.2232\n",
            "Epoch 812/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7336 - val_loss: 4.9533\n",
            "Epoch 813/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.8787 - val_loss: 6.1636\n",
            "Epoch 814/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.1437 - val_loss: 7.9138\n",
            "Epoch 815/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.5904 - val_loss: 3.4115\n",
            "Epoch 816/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7975 - val_loss: 4.4387\n",
            "Epoch 817/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.4167 - val_loss: 9.3227\n",
            "Epoch 818/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.5423 - val_loss: 3.2514\n",
            "Epoch 819/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.9130 - val_loss: 4.2836\n",
            "Epoch 820/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2672 - val_loss: 6.2758\n",
            "Epoch 821/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.3826 - val_loss: 7.9206\n",
            "Epoch 822/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.9908 - val_loss: 6.6121\n",
            "Epoch 823/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.1252 - val_loss: 6.3151\n",
            "Epoch 824/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.7017 - val_loss: 4.5331\n",
            "Epoch 825/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.0072 - val_loss: 5.6224\n",
            "Epoch 826/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3325 - val_loss: 8.4056\n",
            "Epoch 827/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3779 - val_loss: 2.8010\n",
            "Epoch 828/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.3825 - val_loss: 3.1248\n",
            "Epoch 829/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.2480 - val_loss: 6.3529\n",
            "Epoch 830/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.6038 - val_loss: 4.7593\n",
            "Epoch 831/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1801 - val_loss: 6.9418\n",
            "Epoch 832/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5151 - val_loss: 6.5934\n",
            "Epoch 833/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.5164 - val_loss: 6.3921\n",
            "Epoch 834/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.2050 - val_loss: 8.3765\n",
            "Epoch 835/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.3842 - val_loss: 5.5108\n",
            "Epoch 836/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.7047 - val_loss: 4.0588\n",
            "Epoch 837/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.8673 - val_loss: 6.1939\n",
            "Epoch 838/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 9.2257 - val_loss: 6.8411\n",
            "Epoch 839/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.4255 - val_loss: 3.1629\n",
            "Epoch 840/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.4065 - val_loss: 7.6810\n",
            "Epoch 841/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.2592 - val_loss: 3.3017\n",
            "Epoch 842/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.5378 - val_loss: 3.9247\n",
            "Epoch 843/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.6566 - val_loss: 4.5572\n",
            "Epoch 844/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.6281 - val_loss: 5.9355\n",
            "Epoch 845/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.0131 - val_loss: 2.3472\n",
            "Epoch 846/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.7379 - val_loss: 4.8351\n",
            "Epoch 847/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.4483 - val_loss: 6.6133\n",
            "Epoch 848/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1578 - val_loss: 4.0410\n",
            "Epoch 849/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.3194 - val_loss: 3.2171\n",
            "Epoch 850/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.4504 - val_loss: 3.9934\n",
            "Epoch 851/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.2897 - val_loss: 3.6355\n",
            "Epoch 852/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.5107 - val_loss: 6.9617\n",
            "Epoch 853/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.0202 - val_loss: 3.0701\n",
            "Epoch 854/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.6618 - val_loss: 8.1640\n",
            "Epoch 855/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1926 - val_loss: 4.9883\n",
            "Epoch 856/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3844 - val_loss: 7.9468\n",
            "Epoch 857/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.3870 - val_loss: 4.1955\n",
            "Epoch 858/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.4701 - val_loss: 3.9741\n",
            "Epoch 859/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 9.5370 - val_loss: 8.1229\n",
            "Epoch 860/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.3752 - val_loss: 6.3276\n",
            "Epoch 861/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.4744 - val_loss: 3.5482\n",
            "Epoch 862/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1675 - val_loss: 3.6097\n",
            "Epoch 863/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.0123 - val_loss: 4.1440\n",
            "Epoch 864/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.5726 - val_loss: 5.5774\n",
            "Epoch 865/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.0333 - val_loss: 6.3170\n",
            "Epoch 866/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.4275 - val_loss: 4.6140\n",
            "Epoch 867/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1410 - val_loss: 3.9525\n",
            "Epoch 868/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.4115 - val_loss: 3.5878\n",
            "Epoch 869/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.5848 - val_loss: 5.6293\n",
            "Epoch 870/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.9388 - val_loss: 3.4347\n",
            "Epoch 871/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.4719 - val_loss: 4.5579\n",
            "Epoch 872/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.6369 - val_loss: 4.6459\n",
            "Epoch 873/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.4506 - val_loss: 4.2507\n",
            "Epoch 874/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.3721 - val_loss: 3.6529\n",
            "Epoch 875/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.8684 - val_loss: 6.0932\n",
            "Epoch 876/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.6575 - val_loss: 5.1285\n",
            "Epoch 877/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.0353 - val_loss: 4.4144\n",
            "Epoch 878/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.2124 - val_loss: 4.3059\n",
            "Epoch 879/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.0960 - val_loss: 3.5030\n",
            "Epoch 880/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.8605 - val_loss: 5.9351\n",
            "Epoch 881/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 9.8044 - val_loss: 2.9922\n",
            "Epoch 882/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.4313 - val_loss: 5.3079\n",
            "Epoch 883/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.7423 - val_loss: 3.7376\n",
            "Epoch 884/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.6568 - val_loss: 8.5931\n",
            "Epoch 885/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3463 - val_loss: 2.6706\n",
            "Epoch 886/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.9939 - val_loss: 9.7621\n",
            "Epoch 887/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.8923 - val_loss: 10.2412\n",
            "Epoch 888/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.1889 - val_loss: 4.7107\n",
            "Epoch 889/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.6917 - val_loss: 5.8144\n",
            "Epoch 890/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.9588 - val_loss: 7.0854\n",
            "Epoch 891/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.2869 - val_loss: 4.9859\n",
            "Epoch 892/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.2002 - val_loss: 5.3900\n",
            "Epoch 893/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.7478 - val_loss: 5.6289\n",
            "Epoch 894/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.0573 - val_loss: 4.2669\n",
            "Epoch 895/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.2880 - val_loss: 9.6504\n",
            "Epoch 896/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.1196 - val_loss: 6.2817\n",
            "Epoch 897/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.2019 - val_loss: 3.6435\n",
            "Epoch 898/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.4721 - val_loss: 5.8305\n",
            "Epoch 899/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.0383 - val_loss: 8.5281\n",
            "Epoch 900/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.3022 - val_loss: 4.0910\n",
            "Epoch 901/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.4711 - val_loss: 3.2234\n",
            "Epoch 902/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.9249 - val_loss: 5.2564\n",
            "Epoch 903/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 19.1082 - val_loss: 7.2047\n",
            "Epoch 904/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.9033 - val_loss: 7.1425\n",
            "Epoch 905/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.1905 - val_loss: 4.6367\n",
            "Epoch 906/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.1050 - val_loss: 9.9558\n",
            "Epoch 907/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.3980 - val_loss: 5.6110\n",
            "Epoch 908/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.7762 - val_loss: 4.2560\n",
            "Epoch 909/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.9571 - val_loss: 5.1566\n",
            "Epoch 910/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.2149 - val_loss: 6.1163\n",
            "Epoch 911/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.8331 - val_loss: 3.1464\n",
            "Epoch 912/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.2264 - val_loss: 5.2140\n",
            "Epoch 913/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.3174 - val_loss: 6.6147\n",
            "Epoch 914/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.2708 - val_loss: 3.1182\n",
            "Epoch 915/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5784 - val_loss: 3.2332\n",
            "Epoch 916/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.4117 - val_loss: 2.7371\n",
            "Epoch 917/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.3482 - val_loss: 3.4866\n",
            "Epoch 918/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.2726 - val_loss: 4.1294\n",
            "Epoch 919/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.9997 - val_loss: 4.8810\n",
            "Epoch 920/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.5984 - val_loss: 4.0159\n",
            "Epoch 921/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.8964 - val_loss: 5.4638\n",
            "Epoch 922/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2211 - val_loss: 2.8972\n",
            "Epoch 923/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 17.3804 - val_loss: 3.0830\n",
            "Epoch 924/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.3975 - val_loss: 9.6156\n",
            "Epoch 925/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.6340 - val_loss: 3.1684\n",
            "Epoch 926/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.7460 - val_loss: 4.2880\n",
            "Epoch 927/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.0930 - val_loss: 5.4194\n",
            "Epoch 928/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.6907 - val_loss: 6.8218\n",
            "Epoch 929/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.1910 - val_loss: 3.8762\n",
            "Epoch 930/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.4849 - val_loss: 4.0088\n",
            "Epoch 931/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.3422 - val_loss: 6.6028\n",
            "Epoch 932/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.2715 - val_loss: 7.0971\n",
            "Epoch 933/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.3306 - val_loss: 3.9784\n",
            "Epoch 934/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.7538 - val_loss: 4.1798\n",
            "Epoch 935/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.9745 - val_loss: 3.7247\n",
            "Epoch 936/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.2095 - val_loss: 5.1067\n",
            "Epoch 937/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.8051 - val_loss: 10.7733\n",
            "Epoch 938/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.2909 - val_loss: 5.5040\n",
            "Epoch 939/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.9330 - val_loss: 2.9193\n",
            "Epoch 940/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.4871 - val_loss: 4.4715\n",
            "Epoch 941/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7340 - val_loss: 3.2798\n",
            "Epoch 942/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.5415 - val_loss: 3.8623\n",
            "Epoch 943/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.4939 - val_loss: 5.1494\n",
            "Epoch 944/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1220 - val_loss: 4.8127\n",
            "Epoch 945/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.3525 - val_loss: 5.7959\n",
            "Epoch 946/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 9.6468 - val_loss: 3.1816\n",
            "Epoch 947/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.9427 - val_loss: 2.4901\n",
            "Epoch 948/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.1544 - val_loss: 3.1875\n",
            "Epoch 949/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.6968 - val_loss: 3.4371\n",
            "Epoch 950/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.5663 - val_loss: 5.4362\n",
            "Epoch 951/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.0369 - val_loss: 6.2142\n",
            "Epoch 952/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.6804 - val_loss: 6.6273\n",
            "Epoch 953/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.8898 - val_loss: 3.6729\n",
            "Epoch 954/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.1264 - val_loss: 4.9930\n",
            "Epoch 955/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.0144 - val_loss: 6.6122\n",
            "Epoch 956/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 21.3899 - val_loss: 5.5693\n",
            "Epoch 957/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.3822 - val_loss: 6.0793\n",
            "Epoch 958/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.4813 - val_loss: 9.4077\n",
            "Epoch 959/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.4669 - val_loss: 4.5121\n",
            "Epoch 960/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.6770 - val_loss: 7.8414\n",
            "Epoch 961/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.2138 - val_loss: 2.7885\n",
            "Epoch 962/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.7042 - val_loss: 5.2175\n",
            "Epoch 963/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2350 - val_loss: 4.8349\n",
            "Epoch 964/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.8881 - val_loss: 3.9321\n",
            "Epoch 965/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.4906 - val_loss: 3.7448\n",
            "Epoch 966/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.3874 - val_loss: 6.0580\n",
            "Epoch 967/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.2327 - val_loss: 3.6173\n",
            "Epoch 968/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5118 - val_loss: 5.0801\n",
            "Epoch 969/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.2238 - val_loss: 8.7043\n",
            "Epoch 970/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.2565 - val_loss: 4.8649\n",
            "Epoch 971/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.5142 - val_loss: 11.7587\n",
            "Epoch 972/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.2598 - val_loss: 8.4401\n",
            "Epoch 973/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.3804 - val_loss: 3.2152\n",
            "Epoch 974/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.7817 - val_loss: 4.0920\n",
            "Epoch 975/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.8191 - val_loss: 6.4170\n",
            "Epoch 976/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.5261 - val_loss: 4.3651\n",
            "Epoch 977/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.2984 - val_loss: 7.9313\n",
            "Epoch 978/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.8159 - val_loss: 7.6562\n",
            "Epoch 979/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.1449 - val_loss: 4.4918\n",
            "Epoch 980/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.7277 - val_loss: 3.5784\n",
            "Epoch 981/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.9660 - val_loss: 3.7133\n",
            "Epoch 982/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.0381 - val_loss: 4.4583\n",
            "Epoch 983/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.9394 - val_loss: 8.6766\n",
            "Epoch 984/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 16.5239 - val_loss: 8.9928\n",
            "Epoch 985/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.5053 - val_loss: 6.8156\n",
            "Epoch 986/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.7482 - val_loss: 8.2146\n",
            "Epoch 987/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.2739 - val_loss: 5.3120\n",
            "Epoch 988/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 10.0082 - val_loss: 6.3986\n",
            "Epoch 989/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.2605 - val_loss: 9.6437\n",
            "Epoch 990/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.7191 - val_loss: 6.1608\n",
            "Epoch 991/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 12.2675 - val_loss: 3.8105\n",
            "Epoch 992/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 14.1901 - val_loss: 6.8291\n",
            "Epoch 993/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 7.4696 - val_loss: 6.5652\n",
            "Epoch 994/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 18.1270 - val_loss: 8.0326\n",
            "Epoch 995/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 11.9421 - val_loss: 3.7578\n",
            "Epoch 996/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 9.4343 - val_loss: 7.8395\n",
            "Epoch 997/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.9442 - val_loss: 4.2946\n",
            "Epoch 998/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 8.6379 - val_loss: 5.6762\n",
            "Epoch 999/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 15.9109 - val_loss: 4.1226\n",
            "Epoch 1000/1000\n",
            "72/72 [==============================] - 0s 2ms/step - loss: 13.9517 - val_loss: 4.6298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMTuMTQTRsrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "9c3d6fb1-6872-4507-a315-8ee3a5a5bfa1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5HdwjWTpSR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test = pd.read_csv(path_test)\n",
        "ip_test_xt = test['XT'].values \n",
        "ip_test_at = test['AT'].values\n",
        "test_output = test['TOTAL_AT'].values\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_input=[]\n",
        "y_output =[]\n",
        "\n",
        "data_input2D = []\n",
        "data_output1D = []\n",
        "steps = 3\n",
        "\n",
        "for i in range(0,len(ip_test_xt)-steps+1):\n",
        "  temp_3D = []\n",
        "  for k in range(i, i+steps):\n",
        "    temp_2D = []\n",
        "    temp_2D.append(ip_test_xt[k])\n",
        "    temp_2D.append(ip_test_at[k])\n",
        "    \n",
        "    temp_3D.append(temp_2D)\n",
        "\n",
        "  data_input2D.append(temp_3D)\n",
        "x_input.append(data_input2D)\n",
        "\n",
        "\n",
        "x_input = np.concatenate( x_input, axis=0 )\n",
        "\n",
        "\n",
        "x_test = x_input.reshape(-1,3,2)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-lsULcxrngV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "f460b526-b1de-4001-95c2-e33731dcca78"
      },
      "source": [
        "result = model.predict(x_test)\n",
        "\n",
        "test_output = test_output[:result.size]\n",
        "\n",
        "plt.plot(result,'o', color='r')\n",
        "plt.plot(test_output,'x', color='b')\n",
        "plt.title(\"Mô hình K61\")\n",
        "plt.xlabel(\"STT\")\n",
        "plt.ylabel(\"Time\")\n",
        "print(\"actually: \",test_output)\n",
        "print(\"model: \", result.reshape(result.size))\n",
        "percent = []\n",
        "acc = 0\n",
        "for i in range(0,result.size):\n",
        "  p = (1- abs(test_output[i]-result[i])/result[i])*100\n",
        "  percent.append(p)\n",
        "  acc += p\n",
        "print(\"pecent: \", percent)\n",
        "print(\"accuracy: \", acc/(result.size))\n",
        "\n",
        "plt.legend(('prediction', 'reality'),loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actually:  [21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21]\n",
            "model:  [19.690655 20.41974  20.786774 20.998981 21.066048 21.119406 21.18475\n",
            " 21.18665  21.182825 21.179874 21.209576 21.24573  21.312983 21.39874\n",
            " 21.494642 21.584366]\n",
            "pecent:  [array([93.350426], dtype=float32), array([97.15834], dtype=float32), array([98.97422], dtype=float32), array([99.99515], dtype=float32), array([99.68647], dtype=float32), array([99.434616], dtype=float32), array([99.127914], dtype=float32), array([99.119026], dtype=float32), array([99.13692], dtype=float32), array([99.15073], dtype=float32), array([99.01188], dtype=float32), array([98.8434], dtype=float32), array([98.531494], dtype=float32), array([98.13661], dtype=float32), array([97.69877], dtype=float32), array([97.29264], dtype=float32)]\n",
            "accuracy:  [98.41553]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}