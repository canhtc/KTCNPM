{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " K60: XT +AT => Total AT",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/canhtc/KTCNPM/blob/master/K60_XT_%2BAT_%3D%3E_Total_AT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGE_12GJo9f8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adac2060-8de0-47d7-b27d-187b7d819947"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/driver')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/driver; to attempt to forcibly remount, call drive.mount(\"/content/driver\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZgyetqPo_Rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from statsmodels.tools.eval_measures import rmse\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dropout\n",
        "from tensorflow import keras\n",
        "import warnings\n",
        "from glob import glob\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.optimizers import Adam\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wqlLkR7__tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path_train = '/content/driver/My Drive/Colab data/ktcnpm_data/k61/train/*.csv'\n",
        "# path_test = '/content/driver/My Drive/Colab data/ktcnpm_data/k61/test/data_10.csv'\n",
        "\n",
        "# path_train = '/content/driver/My Drive/Colab data/ktcnpm_data/k60/datacsv/train/*.csv'\n",
        "# path_test = '/content/driver/My Drive/Colab data/ktcnpm_data/k60/datacsv/test/data_6.csv'\n",
        "\n",
        "path_train = '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/*.csv'\n",
        "path_test = '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/test_k59/dataH.csv'\n",
        "\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYcDbhUWpDDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "1365d8a2-7320-4de8-f55b-89adcae93e46"
      },
      "source": [
        "\n",
        "files = glob(path_train)\n",
        "x_input =[]\n",
        "y_output=[]\n",
        "print(files)\n",
        "for file in files:\n",
        "  data = pd.read_csv(file)\n",
        "  data_xt = data['XT'].values\n",
        "  data_at = data['AT'].values\n",
        "  data_total_at = data['TOTAL_AT'].values\n",
        "\n",
        "  # data = []\n",
        "  # data.append(data_xt)\n",
        "  # data.append(data_ac)\n",
        "  # data.append(data_total_ac)\n",
        "\n",
        "  # data = np.asarray(data)\n",
        "\n",
        "  # scaler = StandardScaler()\n",
        "\n",
        "  \n",
        "  # data = scaler.fit_transform(data)\n",
        "\n",
        "  # data_xt = data[0]\n",
        "  # data_ac = data[1]\n",
        "  # data_total_ac = data[2]\n",
        "\n",
        "  # print(data_total_at)\n",
        "  # scaler = MinMaxScaler()\n",
        "  \n",
        "\n",
        "#dung de lay du lieu theo cap\n",
        "  data_input2D = []\n",
        "  data_output1D = []\n",
        "  steps = 3\n",
        "  \n",
        "  for i in range(0,len(data_xt)-steps+1):\n",
        "    temp_3D = []\n",
        "    for k in range(i, i+steps):\n",
        "      temp_2D = []\n",
        "      temp_2D.append(data_xt[k])\n",
        "      temp_2D.append(data_at[k])\n",
        "      \n",
        "      temp_3D.append(temp_2D)\n",
        "\n",
        "    data_output1D.append(data_total_at[i])\n",
        "\n",
        "    data_input2D.append(temp_3D)\n",
        "  x_input.append(data_input2D)\n",
        "  y_output.append(data_output1D)\n",
        "\n",
        "x_input = np.concatenate( x_input, axis=0 )\n",
        "y_output = np.concatenate( y_output, axis=0 )\n",
        "x_train = np.asarray(x_input)\n",
        "y_train = np.asarray(y_output)\n",
        "\n",
        "x_train = x_train.reshape(-1,3,2)\n",
        "y_train = y_train.reshape(-1,1)\n",
        "# print( x_train.shape, y_train.shape)\n",
        "print (\"INPUT:\",x_train[:2])\n",
        "print(\"OUTPUT:\",y_train[:2])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataA.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataB.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataC.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataD.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataE.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataF.csv', '/content/driver/My Drive/Colab data/ktcnpm_data/k59/datacsv/train_k59/dataG.csv']\n",
            "INPUT: [[[0.13 4.  ]\n",
            "  [0.17 5.  ]\n",
            "  [0.24 6.  ]]\n",
            "\n",
            " [[0.17 5.  ]\n",
            "  [0.24 6.  ]\n",
            "  [0.29 7.  ]]]\n",
            "OUTPUT: [[22]\n",
            " [22]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gzqAZa8pFNw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "96c9a68e-f87b-4db4-d65b-8cb30a3fe1f0"
      },
      "source": [
        "n_input = 3\n",
        "n_features = 2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(10, activation='relu', input_shape=(n_input, n_features)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1))\n",
        "model.summary()\n",
        "# adam = Adam(lr=0.001) \n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_6 (LSTM)                (None, 10)                520       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 531\n",
            "Trainable params: 531\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hO1MD5mpHZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "205bf33a-677e-46fb-9add-72eb16505fd7"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=1000, validation_split=0.1, verbose=1, batch_size=3)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81 samples, validate on 10 samples\n",
            "Epoch 1/1000\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 339.8471 - val_loss: 375.4874\n",
            "Epoch 2/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 327.9106 - val_loss: 340.7282\n",
            "Epoch 3/1000\n",
            "81/81 [==============================] - 0s 829us/step - loss: 288.4324 - val_loss: 275.3687\n",
            "Epoch 4/1000\n",
            "81/81 [==============================] - 0s 875us/step - loss: 240.5481 - val_loss: 177.7776\n",
            "Epoch 5/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 170.6251 - val_loss: 48.7442\n",
            "Epoch 6/1000\n",
            "81/81 [==============================] - 0s 875us/step - loss: 124.1699 - val_loss: 19.0151\n",
            "Epoch 7/1000\n",
            "81/81 [==============================] - 0s 993us/step - loss: 120.4366 - val_loss: 30.7851\n",
            "Epoch 8/1000\n",
            "81/81 [==============================] - 0s 910us/step - loss: 105.2427 - val_loss: 17.8283\n",
            "Epoch 9/1000\n",
            "81/81 [==============================] - 0s 887us/step - loss: 103.4454 - val_loss: 18.8712\n",
            "Epoch 10/1000\n",
            "81/81 [==============================] - 0s 880us/step - loss: 100.4999 - val_loss: 19.9964\n",
            "Epoch 11/1000\n",
            "81/81 [==============================] - 0s 975us/step - loss: 109.2316 - val_loss: 11.7691\n",
            "Epoch 12/1000\n",
            "81/81 [==============================] - 0s 909us/step - loss: 88.2937 - val_loss: 18.5962\n",
            "Epoch 13/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 101.7937 - val_loss: 11.0191\n",
            "Epoch 14/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 113.4098 - val_loss: 9.1526\n",
            "Epoch 15/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 97.5924 - val_loss: 9.5543\n",
            "Epoch 16/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 89.2536 - val_loss: 9.1478\n",
            "Epoch 17/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 115.5390 - val_loss: 6.9826\n",
            "Epoch 18/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 66.5185 - val_loss: 7.0624\n",
            "Epoch 19/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 94.2495 - val_loss: 6.2877\n",
            "Epoch 20/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 99.0069 - val_loss: 8.0002\n",
            "Epoch 21/1000\n",
            "81/81 [==============================] - 0s 931us/step - loss: 85.3763 - val_loss: 5.4092\n",
            "Epoch 22/1000\n",
            "81/81 [==============================] - 0s 860us/step - loss: 94.8457 - val_loss: 7.7919\n",
            "Epoch 23/1000\n",
            "81/81 [==============================] - 0s 888us/step - loss: 75.3596 - val_loss: 5.9217\n",
            "Epoch 24/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 95.9098 - val_loss: 5.7091\n",
            "Epoch 25/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 74.0973 - val_loss: 2.6838\n",
            "Epoch 26/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 84.3320 - val_loss: 3.7195\n",
            "Epoch 27/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 70.4050 - val_loss: 3.6177\n",
            "Epoch 28/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 77.6566 - val_loss: 4.5407\n",
            "Epoch 29/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 89.7611 - val_loss: 13.5003\n",
            "Epoch 30/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 90.9330 - val_loss: 6.1330\n",
            "Epoch 31/1000\n",
            "81/81 [==============================] - 0s 877us/step - loss: 100.0443 - val_loss: 3.7498\n",
            "Epoch 32/1000\n",
            "81/81 [==============================] - 0s 835us/step - loss: 75.6938 - val_loss: 8.9653\n",
            "Epoch 33/1000\n",
            "81/81 [==============================] - 0s 822us/step - loss: 94.5550 - val_loss: 6.4469\n",
            "Epoch 34/1000\n",
            "81/81 [==============================] - 0s 816us/step - loss: 89.0800 - val_loss: 5.8185\n",
            "Epoch 35/1000\n",
            "81/81 [==============================] - 0s 898us/step - loss: 73.1917 - val_loss: 8.0143\n",
            "Epoch 36/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 64.4604 - val_loss: 4.6213\n",
            "Epoch 37/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 74.6603 - val_loss: 9.6356\n",
            "Epoch 38/1000\n",
            "81/81 [==============================] - 0s 874us/step - loss: 62.6519 - val_loss: 5.4894\n",
            "Epoch 39/1000\n",
            "81/81 [==============================] - 0s 869us/step - loss: 70.9027 - val_loss: 3.7135\n",
            "Epoch 40/1000\n",
            "81/81 [==============================] - 0s 891us/step - loss: 91.7515 - val_loss: 4.9278\n",
            "Epoch 41/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 66.8617 - val_loss: 6.4930\n",
            "Epoch 42/1000\n",
            "81/81 [==============================] - 0s 883us/step - loss: 94.6927 - val_loss: 9.1059\n",
            "Epoch 43/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 64.3665 - val_loss: 7.8704\n",
            "Epoch 44/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 69.7751 - val_loss: 5.0558\n",
            "Epoch 45/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 72.5236 - val_loss: 6.3924\n",
            "Epoch 46/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 71.0944 - val_loss: 3.6225\n",
            "Epoch 47/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 68.9214 - val_loss: 5.8795\n",
            "Epoch 48/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 70.2673 - val_loss: 10.8440\n",
            "Epoch 49/1000\n",
            "81/81 [==============================] - 0s 878us/step - loss: 102.5702 - val_loss: 8.5211\n",
            "Epoch 50/1000\n",
            "81/81 [==============================] - 0s 859us/step - loss: 80.9125 - val_loss: 5.3398\n",
            "Epoch 51/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 71.9354 - val_loss: 3.0920\n",
            "Epoch 52/1000\n",
            "81/81 [==============================] - 0s 911us/step - loss: 86.4489 - val_loss: 6.9014\n",
            "Epoch 53/1000\n",
            "81/81 [==============================] - 0s 875us/step - loss: 79.5663 - val_loss: 9.1058\n",
            "Epoch 54/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 87.1711 - val_loss: 14.4905\n",
            "Epoch 55/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 72.8351 - val_loss: 7.9483\n",
            "Epoch 56/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 69.5471 - val_loss: 5.1772\n",
            "Epoch 57/1000\n",
            "81/81 [==============================] - 0s 876us/step - loss: 69.8917 - val_loss: 8.3368\n",
            "Epoch 58/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 67.9592 - val_loss: 8.4694\n",
            "Epoch 59/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 50.4653 - val_loss: 9.6938\n",
            "Epoch 60/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 66.5783 - val_loss: 10.3927\n",
            "Epoch 61/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 68.3914 - val_loss: 8.0766\n",
            "Epoch 62/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 51.5099 - val_loss: 11.3931\n",
            "Epoch 63/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 72.1075 - val_loss: 7.4362\n",
            "Epoch 64/1000\n",
            "81/81 [==============================] - 0s 981us/step - loss: 65.0627 - val_loss: 5.9098\n",
            "Epoch 65/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 62.2672 - val_loss: 15.1102\n",
            "Epoch 66/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 80.7522 - val_loss: 6.8538\n",
            "Epoch 67/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 63.3427 - val_loss: 9.1953\n",
            "Epoch 68/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 59.4021 - val_loss: 11.5598\n",
            "Epoch 69/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 52.4271 - val_loss: 0.8971\n",
            "Epoch 70/1000\n",
            "81/81 [==============================] - 0s 869us/step - loss: 64.3330 - val_loss: 12.5684\n",
            "Epoch 71/1000\n",
            "81/81 [==============================] - 0s 821us/step - loss: 46.6555 - val_loss: 5.3494\n",
            "Epoch 72/1000\n",
            "81/81 [==============================] - 0s 932us/step - loss: 51.3391 - val_loss: 0.5393\n",
            "Epoch 73/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 40.1479 - val_loss: 20.8315\n",
            "Epoch 74/1000\n",
            "81/81 [==============================] - 0s 818us/step - loss: 60.1279 - val_loss: 1.6269\n",
            "Epoch 75/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 39.9114 - val_loss: 8.1894\n",
            "Epoch 76/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 57.1354 - val_loss: 3.0836\n",
            "Epoch 77/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 52.0423 - val_loss: 2.1693\n",
            "Epoch 78/1000\n",
            "81/81 [==============================] - 0s 926us/step - loss: 55.4936 - val_loss: 0.3485\n",
            "Epoch 79/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 59.4962 - val_loss: 6.5268\n",
            "Epoch 80/1000\n",
            "81/81 [==============================] - 0s 953us/step - loss: 50.4468 - val_loss: 3.4046\n",
            "Epoch 81/1000\n",
            "81/81 [==============================] - 0s 935us/step - loss: 57.8539 - val_loss: 3.6042\n",
            "Epoch 82/1000\n",
            "81/81 [==============================] - 0s 895us/step - loss: 62.4440 - val_loss: 4.9811\n",
            "Epoch 83/1000\n",
            "81/81 [==============================] - 0s 960us/step - loss: 47.1247 - val_loss: 3.4792\n",
            "Epoch 84/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 44.8167 - val_loss: 13.6620\n",
            "Epoch 85/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 61.4572 - val_loss: 6.0897\n",
            "Epoch 86/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 50.5205 - val_loss: 1.7306\n",
            "Epoch 87/1000\n",
            "81/81 [==============================] - 0s 870us/step - loss: 56.1580 - val_loss: 6.7814\n",
            "Epoch 88/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 52.5623 - val_loss: 0.6834\n",
            "Epoch 89/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 68.4612 - val_loss: 12.9360\n",
            "Epoch 90/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 40.3343 - val_loss: 9.1332\n",
            "Epoch 91/1000\n",
            "81/81 [==============================] - 0s 879us/step - loss: 71.7713 - val_loss: 7.7593\n",
            "Epoch 92/1000\n",
            "81/81 [==============================] - 0s 988us/step - loss: 54.9294 - val_loss: 4.5298\n",
            "Epoch 93/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 50.1055 - val_loss: 4.9617\n",
            "Epoch 94/1000\n",
            "81/81 [==============================] - 0s 904us/step - loss: 43.7765 - val_loss: 5.7654\n",
            "Epoch 95/1000\n",
            "81/81 [==============================] - 0s 828us/step - loss: 56.1791 - val_loss: 9.2631\n",
            "Epoch 96/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 55.7092 - val_loss: 4.4923\n",
            "Epoch 97/1000\n",
            "81/81 [==============================] - 0s 860us/step - loss: 46.9177 - val_loss: 7.8132\n",
            "Epoch 98/1000\n",
            "81/81 [==============================] - 0s 828us/step - loss: 59.9012 - val_loss: 1.2742\n",
            "Epoch 99/1000\n",
            "81/81 [==============================] - 0s 824us/step - loss: 43.9732 - val_loss: 7.2342\n",
            "Epoch 100/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 61.3517 - val_loss: 6.4291\n",
            "Epoch 101/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 37.4548 - val_loss: 8.6634\n",
            "Epoch 102/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 60.1356 - val_loss: 6.1036\n",
            "Epoch 103/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 50.4768 - val_loss: 1.2884\n",
            "Epoch 104/1000\n",
            "81/81 [==============================] - 0s 872us/step - loss: 50.5693 - val_loss: 0.8310\n",
            "Epoch 105/1000\n",
            "81/81 [==============================] - 0s 874us/step - loss: 56.6456 - val_loss: 3.8487\n",
            "Epoch 106/1000\n",
            "81/81 [==============================] - 0s 950us/step - loss: 79.9996 - val_loss: 6.0497\n",
            "Epoch 107/1000\n",
            "81/81 [==============================] - 0s 976us/step - loss: 40.0843 - val_loss: 2.3270\n",
            "Epoch 108/1000\n",
            "81/81 [==============================] - 0s 860us/step - loss: 53.7130 - val_loss: 2.4577\n",
            "Epoch 109/1000\n",
            "81/81 [==============================] - 0s 879us/step - loss: 40.9031 - val_loss: 4.2997\n",
            "Epoch 110/1000\n",
            "81/81 [==============================] - 0s 887us/step - loss: 40.0633 - val_loss: 3.3624\n",
            "Epoch 111/1000\n",
            "81/81 [==============================] - 0s 889us/step - loss: 26.8643 - val_loss: 2.5103\n",
            "Epoch 112/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 61.4046 - val_loss: 0.4881\n",
            "Epoch 113/1000\n",
            "81/81 [==============================] - 0s 927us/step - loss: 42.0844 - val_loss: 4.2223\n",
            "Epoch 114/1000\n",
            "81/81 [==============================] - 0s 890us/step - loss: 40.4956 - val_loss: 2.4799\n",
            "Epoch 115/1000\n",
            "81/81 [==============================] - 0s 880us/step - loss: 45.0997 - val_loss: 3.5022\n",
            "Epoch 116/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 57.5635 - val_loss: 2.7069\n",
            "Epoch 117/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 46.0569 - val_loss: 8.3421\n",
            "Epoch 118/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 43.3978 - val_loss: 1.8681\n",
            "Epoch 119/1000\n",
            "81/81 [==============================] - 0s 875us/step - loss: 51.8349 - val_loss: 3.2641\n",
            "Epoch 120/1000\n",
            "81/81 [==============================] - 0s 952us/step - loss: 49.8790 - val_loss: 4.3988\n",
            "Epoch 121/1000\n",
            "81/81 [==============================] - 0s 876us/step - loss: 43.8988 - val_loss: 18.9163\n",
            "Epoch 122/1000\n",
            "81/81 [==============================] - 0s 882us/step - loss: 58.1100 - val_loss: 6.6942\n",
            "Epoch 123/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 48.6268 - val_loss: 2.1905\n",
            "Epoch 124/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 47.4398 - val_loss: 3.0338\n",
            "Epoch 125/1000\n",
            "81/81 [==============================] - 0s 819us/step - loss: 58.4791 - val_loss: 5.2341\n",
            "Epoch 126/1000\n",
            "81/81 [==============================] - 0s 827us/step - loss: 67.9759 - val_loss: 2.9674\n",
            "Epoch 127/1000\n",
            "81/81 [==============================] - 0s 825us/step - loss: 65.8860 - val_loss: 4.5349\n",
            "Epoch 128/1000\n",
            "81/81 [==============================] - 0s 897us/step - loss: 52.5555 - val_loss: 3.8023\n",
            "Epoch 129/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 60.2263 - val_loss: 6.7419\n",
            "Epoch 130/1000\n",
            "81/81 [==============================] - 0s 822us/step - loss: 50.3571 - val_loss: 3.4393\n",
            "Epoch 131/1000\n",
            "81/81 [==============================] - 0s 859us/step - loss: 44.4487 - val_loss: 2.0835\n",
            "Epoch 132/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 41.9654 - val_loss: 4.1450\n",
            "Epoch 133/1000\n",
            "81/81 [==============================] - 0s 875us/step - loss: 43.0411 - val_loss: 6.5401\n",
            "Epoch 134/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 60.5822 - val_loss: 4.8230\n",
            "Epoch 135/1000\n",
            "81/81 [==============================] - 0s 897us/step - loss: 46.4233 - val_loss: 5.3530\n",
            "Epoch 136/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 43.8883 - val_loss: 1.1956\n",
            "Epoch 137/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 41.0455 - val_loss: 7.0068\n",
            "Epoch 138/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 42.0297 - val_loss: 6.4229\n",
            "Epoch 139/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 44.9929 - val_loss: 4.1213\n",
            "Epoch 140/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 55.9768 - val_loss: 1.9981\n",
            "Epoch 141/1000\n",
            "81/81 [==============================] - 0s 902us/step - loss: 60.9039 - val_loss: 4.1342\n",
            "Epoch 142/1000\n",
            "81/81 [==============================] - 0s 882us/step - loss: 33.4134 - val_loss: 6.6103\n",
            "Epoch 143/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 42.0761 - val_loss: 7.6920\n",
            "Epoch 144/1000\n",
            "81/81 [==============================] - 0s 833us/step - loss: 57.4282 - val_loss: 5.1197\n",
            "Epoch 145/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 40.6071 - val_loss: 4.6882\n",
            "Epoch 146/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 37.5976 - val_loss: 8.5362\n",
            "Epoch 147/1000\n",
            "81/81 [==============================] - 0s 904us/step - loss: 48.6690 - val_loss: 4.8624\n",
            "Epoch 148/1000\n",
            "81/81 [==============================] - 0s 876us/step - loss: 40.5865 - val_loss: 10.1718\n",
            "Epoch 149/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 32.3164 - val_loss: 5.0393\n",
            "Epoch 150/1000\n",
            "81/81 [==============================] - 0s 878us/step - loss: 41.3593 - val_loss: 1.0444\n",
            "Epoch 151/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 42.0105 - val_loss: 8.2856\n",
            "Epoch 152/1000\n",
            "81/81 [==============================] - 0s 881us/step - loss: 40.1467 - val_loss: 3.1089\n",
            "Epoch 153/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 61.1967 - val_loss: 16.3354\n",
            "Epoch 154/1000\n",
            "81/81 [==============================] - 0s 950us/step - loss: 56.4844 - val_loss: 1.4391\n",
            "Epoch 155/1000\n",
            "81/81 [==============================] - 0s 881us/step - loss: 40.2571 - val_loss: 1.0150\n",
            "Epoch 156/1000\n",
            "81/81 [==============================] - 0s 889us/step - loss: 52.0865 - val_loss: 11.9540\n",
            "Epoch 157/1000\n",
            "81/81 [==============================] - 0s 976us/step - loss: 30.7933 - val_loss: 8.6777\n",
            "Epoch 158/1000\n",
            "81/81 [==============================] - 0s 961us/step - loss: 46.4773 - val_loss: 3.7392\n",
            "Epoch 159/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 31.2510 - val_loss: 9.5115\n",
            "Epoch 160/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 42.6432 - val_loss: 4.1073\n",
            "Epoch 161/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 31.1394 - val_loss: 10.7701\n",
            "Epoch 162/1000\n",
            "81/81 [==============================] - 0s 984us/step - loss: 44.2218 - val_loss: 2.3335\n",
            "Epoch 163/1000\n",
            "81/81 [==============================] - 0s 947us/step - loss: 51.6907 - val_loss: 7.2559\n",
            "Epoch 164/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 37.6515 - val_loss: 5.0526\n",
            "Epoch 165/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 63.4191 - val_loss: 6.3884\n",
            "Epoch 166/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 33.7916 - val_loss: 4.2224\n",
            "Epoch 167/1000\n",
            "81/81 [==============================] - 0s 835us/step - loss: 34.2190 - val_loss: 2.0752\n",
            "Epoch 168/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 40.3002 - val_loss: 2.4420\n",
            "Epoch 169/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 38.5927 - val_loss: 1.0697\n",
            "Epoch 170/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 37.3848 - val_loss: 4.9328\n",
            "Epoch 171/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 40.3644 - val_loss: 3.1644\n",
            "Epoch 172/1000\n",
            "81/81 [==============================] - 0s 825us/step - loss: 38.8742 - val_loss: 4.2471\n",
            "Epoch 173/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 45.5266 - val_loss: 4.6376\n",
            "Epoch 174/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 44.4782 - val_loss: 3.7423\n",
            "Epoch 175/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 43.9683 - val_loss: 8.0079\n",
            "Epoch 176/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 36.8770 - val_loss: 4.4965\n",
            "Epoch 177/1000\n",
            "81/81 [==============================] - 0s 894us/step - loss: 44.0468 - val_loss: 2.9121\n",
            "Epoch 178/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 42.8690 - val_loss: 2.6420\n",
            "Epoch 179/1000\n",
            "81/81 [==============================] - 0s 836us/step - loss: 33.4583 - val_loss: 7.1295\n",
            "Epoch 180/1000\n",
            "81/81 [==============================] - 0s 879us/step - loss: 35.3318 - val_loss: 3.3108\n",
            "Epoch 181/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 40.9148 - val_loss: 6.1029\n",
            "Epoch 182/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 34.2675 - val_loss: 4.8907\n",
            "Epoch 183/1000\n",
            "81/81 [==============================] - 0s 906us/step - loss: 36.1653 - val_loss: 6.2204\n",
            "Epoch 184/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 31.8821 - val_loss: 2.5066\n",
            "Epoch 185/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 31.9176 - val_loss: 2.1666\n",
            "Epoch 186/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 57.0188 - val_loss: 3.8208\n",
            "Epoch 187/1000\n",
            "81/81 [==============================] - 0s 869us/step - loss: 28.9330 - val_loss: 4.7884\n",
            "Epoch 188/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 49.7691 - val_loss: 2.7252\n",
            "Epoch 189/1000\n",
            "81/81 [==============================] - 0s 940us/step - loss: 34.7175 - val_loss: 8.2424\n",
            "Epoch 190/1000\n",
            "81/81 [==============================] - 0s 921us/step - loss: 31.3331 - val_loss: 3.8024\n",
            "Epoch 191/1000\n",
            "81/81 [==============================] - 0s 959us/step - loss: 44.4507 - val_loss: 5.2406\n",
            "Epoch 192/1000\n",
            "81/81 [==============================] - 0s 899us/step - loss: 37.6921 - val_loss: 1.6988\n",
            "Epoch 193/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 45.3650 - val_loss: 4.4361\n",
            "Epoch 194/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 41.7233 - val_loss: 1.5869\n",
            "Epoch 195/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 40.4204 - val_loss: 5.0463\n",
            "Epoch 196/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 46.9368 - val_loss: 1.8440\n",
            "Epoch 197/1000\n",
            "81/81 [==============================] - 0s 881us/step - loss: 54.3969 - val_loss: 2.8811\n",
            "Epoch 198/1000\n",
            "81/81 [==============================] - 0s 889us/step - loss: 54.2648 - val_loss: 8.5726\n",
            "Epoch 199/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 35.1315 - val_loss: 1.2305\n",
            "Epoch 200/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 40.4534 - val_loss: 5.8508\n",
            "Epoch 201/1000\n",
            "81/81 [==============================] - 0s 879us/step - loss: 45.1130 - val_loss: 3.4931\n",
            "Epoch 202/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 34.5633 - val_loss: 8.6654\n",
            "Epoch 203/1000\n",
            "81/81 [==============================] - 0s 901us/step - loss: 37.3164 - val_loss: 1.3483\n",
            "Epoch 204/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 27.6097 - val_loss: 2.3641\n",
            "Epoch 205/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 40.9422 - val_loss: 8.8287\n",
            "Epoch 206/1000\n",
            "81/81 [==============================] - 0s 997us/step - loss: 36.6639 - val_loss: 1.0180\n",
            "Epoch 207/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 32.3846 - val_loss: 2.4947\n",
            "Epoch 208/1000\n",
            "81/81 [==============================] - 0s 913us/step - loss: 37.9688 - val_loss: 2.2338\n",
            "Epoch 209/1000\n",
            "81/81 [==============================] - 0s 885us/step - loss: 36.3380 - val_loss: 4.2557\n",
            "Epoch 210/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 30.8176 - val_loss: 3.9100\n",
            "Epoch 211/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 44.6915 - val_loss: 12.4827\n",
            "Epoch 212/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 53.1215 - val_loss: 1.5608\n",
            "Epoch 213/1000\n",
            "81/81 [==============================] - 0s 919us/step - loss: 44.1736 - val_loss: 5.3047\n",
            "Epoch 214/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 49.0574 - val_loss: 6.7366\n",
            "Epoch 215/1000\n",
            "81/81 [==============================] - 0s 813us/step - loss: 45.6836 - val_loss: 1.3904\n",
            "Epoch 216/1000\n",
            "81/81 [==============================] - 0s 820us/step - loss: 31.6450 - val_loss: 10.0814\n",
            "Epoch 217/1000\n",
            "81/81 [==============================] - 0s 827us/step - loss: 58.9866 - val_loss: 4.5390\n",
            "Epoch 218/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 42.1175 - val_loss: 1.6865\n",
            "Epoch 219/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 30.1559 - val_loss: 7.5342\n",
            "Epoch 220/1000\n",
            "81/81 [==============================] - 0s 947us/step - loss: 40.9986 - val_loss: 7.2856\n",
            "Epoch 221/1000\n",
            "81/81 [==============================] - 0s 815us/step - loss: 29.1662 - val_loss: 3.3924\n",
            "Epoch 222/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 43.7402 - val_loss: 5.4847\n",
            "Epoch 223/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 48.4112 - val_loss: 11.4969\n",
            "Epoch 224/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 43.0198 - val_loss: 7.1856\n",
            "Epoch 225/1000\n",
            "81/81 [==============================] - 0s 877us/step - loss: 27.7985 - val_loss: 4.5137\n",
            "Epoch 226/1000\n",
            "81/81 [==============================] - 0s 813us/step - loss: 34.3061 - val_loss: 12.5553\n",
            "Epoch 227/1000\n",
            "81/81 [==============================] - 0s 826us/step - loss: 31.3489 - val_loss: 9.5279\n",
            "Epoch 228/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 33.7038 - val_loss: 3.2232\n",
            "Epoch 229/1000\n",
            "81/81 [==============================] - 0s 828us/step - loss: 33.1575 - val_loss: 8.4924\n",
            "Epoch 230/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 36.3633 - val_loss: 3.2349\n",
            "Epoch 231/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 41.0829 - val_loss: 8.2167\n",
            "Epoch 232/1000\n",
            "81/81 [==============================] - 0s 975us/step - loss: 43.8726 - val_loss: 4.8847\n",
            "Epoch 233/1000\n",
            "81/81 [==============================] - 0s 879us/step - loss: 27.3535 - val_loss: 2.0940\n",
            "Epoch 234/1000\n",
            "81/81 [==============================] - 0s 944us/step - loss: 45.2864 - val_loss: 4.0581\n",
            "Epoch 235/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 40.1353 - val_loss: 6.1193\n",
            "Epoch 236/1000\n",
            "81/81 [==============================] - 0s 930us/step - loss: 45.2529 - val_loss: 10.5554\n",
            "Epoch 237/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 43.3969 - val_loss: 8.4010\n",
            "Epoch 238/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 43.4907 - val_loss: 1.4174\n",
            "Epoch 239/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 46.6724 - val_loss: 7.9139\n",
            "Epoch 240/1000\n",
            "81/81 [==============================] - 0s 913us/step - loss: 33.7239 - val_loss: 7.5698\n",
            "Epoch 241/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 39.4638 - val_loss: 2.9185\n",
            "Epoch 242/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 37.4381 - val_loss: 2.4685\n",
            "Epoch 243/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 28.5336 - val_loss: 5.1003\n",
            "Epoch 244/1000\n",
            "81/81 [==============================] - 0s 880us/step - loss: 44.2899 - val_loss: 9.3459\n",
            "Epoch 245/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 35.0975 - val_loss: 5.8266\n",
            "Epoch 246/1000\n",
            "81/81 [==============================] - 0s 890us/step - loss: 43.9139 - val_loss: 8.9820\n",
            "Epoch 247/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 46.5408 - val_loss: 4.8202\n",
            "Epoch 248/1000\n",
            "81/81 [==============================] - 0s 952us/step - loss: 37.0352 - val_loss: 4.1806\n",
            "Epoch 249/1000\n",
            "81/81 [==============================] - 0s 908us/step - loss: 33.9715 - val_loss: 3.2474\n",
            "Epoch 250/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 36.2903 - val_loss: 13.6801\n",
            "Epoch 251/1000\n",
            "81/81 [==============================] - 0s 870us/step - loss: 39.7308 - val_loss: 3.8769\n",
            "Epoch 252/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 34.3238 - val_loss: 3.7873\n",
            "Epoch 253/1000\n",
            "81/81 [==============================] - 0s 888us/step - loss: 38.3311 - val_loss: 2.2734\n",
            "Epoch 254/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 34.9899 - val_loss: 5.1420\n",
            "Epoch 255/1000\n",
            "81/81 [==============================] - 0s 813us/step - loss: 37.0746 - val_loss: 2.6244\n",
            "Epoch 256/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 38.1296 - val_loss: 4.6257\n",
            "Epoch 257/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 35.2245 - val_loss: 3.4421\n",
            "Epoch 258/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 32.3962 - val_loss: 5.6616\n",
            "Epoch 259/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 38.7918 - val_loss: 3.2906\n",
            "Epoch 260/1000\n",
            "81/81 [==============================] - 0s 872us/step - loss: 32.8634 - val_loss: 12.0312\n",
            "Epoch 261/1000\n",
            "81/81 [==============================] - 0s 880us/step - loss: 39.3598 - val_loss: 4.6466\n",
            "Epoch 262/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 40.6363 - val_loss: 3.1388\n",
            "Epoch 263/1000\n",
            "81/81 [==============================] - 0s 916us/step - loss: 38.7090 - val_loss: 8.4810\n",
            "Epoch 264/1000\n",
            "81/81 [==============================] - 0s 940us/step - loss: 42.0532 - val_loss: 7.2346\n",
            "Epoch 265/1000\n",
            "81/81 [==============================] - 0s 886us/step - loss: 30.3724 - val_loss: 4.5680\n",
            "Epoch 266/1000\n",
            "81/81 [==============================] - 0s 817us/step - loss: 31.4052 - val_loss: 3.0550\n",
            "Epoch 267/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 44.7029 - val_loss: 10.7282\n",
            "Epoch 268/1000\n",
            "81/81 [==============================] - 0s 835us/step - loss: 45.0986 - val_loss: 1.7055\n",
            "Epoch 269/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 47.6905 - val_loss: 6.5602\n",
            "Epoch 270/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 32.6025 - val_loss: 8.9799\n",
            "Epoch 271/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 38.0705 - val_loss: 5.5829\n",
            "Epoch 272/1000\n",
            "81/81 [==============================] - 0s 928us/step - loss: 36.9310 - val_loss: 9.0734\n",
            "Epoch 273/1000\n",
            "81/81 [==============================] - 0s 876us/step - loss: 41.7649 - val_loss: 3.8857\n",
            "Epoch 274/1000\n",
            "81/81 [==============================] - 0s 829us/step - loss: 41.2615 - val_loss: 5.6743\n",
            "Epoch 275/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 52.7887 - val_loss: 5.6907\n",
            "Epoch 276/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 49.5210 - val_loss: 3.9501\n",
            "Epoch 277/1000\n",
            "81/81 [==============================] - 0s 920us/step - loss: 28.8570 - val_loss: 8.9198\n",
            "Epoch 278/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 40.8419 - val_loss: 3.7030\n",
            "Epoch 279/1000\n",
            "81/81 [==============================] - 0s 893us/step - loss: 46.8520 - val_loss: 4.7068\n",
            "Epoch 280/1000\n",
            "81/81 [==============================] - 0s 887us/step - loss: 34.8667 - val_loss: 8.1463\n",
            "Epoch 281/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 43.7158 - val_loss: 3.3629\n",
            "Epoch 282/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 30.1630 - val_loss: 4.0603\n",
            "Epoch 283/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 30.5837 - val_loss: 7.4382\n",
            "Epoch 284/1000\n",
            "81/81 [==============================] - 0s 877us/step - loss: 37.8406 - val_loss: 4.9029\n",
            "Epoch 285/1000\n",
            "81/81 [==============================] - 0s 926us/step - loss: 30.5976 - val_loss: 5.2564\n",
            "Epoch 286/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 23.1249 - val_loss: 4.1172\n",
            "Epoch 287/1000\n",
            "81/81 [==============================] - 0s 877us/step - loss: 41.9004 - val_loss: 9.8706\n",
            "Epoch 288/1000\n",
            "81/81 [==============================] - 0s 882us/step - loss: 45.8241 - val_loss: 2.4930\n",
            "Epoch 289/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 40.3965 - val_loss: 9.6012\n",
            "Epoch 290/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 31.4914 - val_loss: 2.6862\n",
            "Epoch 291/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 31.8713 - val_loss: 7.6708\n",
            "Epoch 292/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 37.1952 - val_loss: 4.4676\n",
            "Epoch 293/1000\n",
            "81/81 [==============================] - 0s 872us/step - loss: 37.0170 - val_loss: 3.8236\n",
            "Epoch 294/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 39.9916 - val_loss: 7.4731\n",
            "Epoch 295/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 41.7811 - val_loss: 1.4519\n",
            "Epoch 296/1000\n",
            "81/81 [==============================] - 0s 878us/step - loss: 29.9141 - val_loss: 5.9922\n",
            "Epoch 297/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 47.1096 - val_loss: 2.3439\n",
            "Epoch 298/1000\n",
            "81/81 [==============================] - 0s 920us/step - loss: 30.4779 - val_loss: 8.6555\n",
            "Epoch 299/1000\n",
            "81/81 [==============================] - 0s 871us/step - loss: 30.4983 - val_loss: 3.2490\n",
            "Epoch 300/1000\n",
            "81/81 [==============================] - 0s 888us/step - loss: 40.4334 - val_loss: 2.6914\n",
            "Epoch 301/1000\n",
            "81/81 [==============================] - 0s 901us/step - loss: 36.3798 - val_loss: 9.4562\n",
            "Epoch 302/1000\n",
            "81/81 [==============================] - 0s 918us/step - loss: 36.7153 - val_loss: 3.3906\n",
            "Epoch 303/1000\n",
            "81/81 [==============================] - 0s 879us/step - loss: 27.4950 - val_loss: 6.5382\n",
            "Epoch 304/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 36.6917 - val_loss: 1.2932\n",
            "Epoch 305/1000\n",
            "81/81 [==============================] - 0s 940us/step - loss: 39.2600 - val_loss: 2.8985\n",
            "Epoch 306/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 26.8715 - val_loss: 5.6641\n",
            "Epoch 307/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 30.7120 - val_loss: 4.0194\n",
            "Epoch 308/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 39.4544 - val_loss: 2.1332\n",
            "Epoch 309/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 28.3020 - val_loss: 11.1448\n",
            "Epoch 310/1000\n",
            "81/81 [==============================] - 0s 872us/step - loss: 38.9350 - val_loss: 4.1591\n",
            "Epoch 311/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 34.8135 - val_loss: 2.4909\n",
            "Epoch 312/1000\n",
            "81/81 [==============================] - 0s 827us/step - loss: 33.3201 - val_loss: 5.4732\n",
            "Epoch 313/1000\n",
            "81/81 [==============================] - 0s 894us/step - loss: 35.5584 - val_loss: 1.8260\n",
            "Epoch 314/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 33.2678 - val_loss: 4.5690\n",
            "Epoch 315/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 39.9989 - val_loss: 6.4170\n",
            "Epoch 316/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 29.9025 - val_loss: 10.3358\n",
            "Epoch 317/1000\n",
            "81/81 [==============================] - 0s 957us/step - loss: 40.9745 - val_loss: 2.1499\n",
            "Epoch 318/1000\n",
            "81/81 [==============================] - 0s 921us/step - loss: 27.5846 - val_loss: 4.1445\n",
            "Epoch 319/1000\n",
            "81/81 [==============================] - 0s 909us/step - loss: 32.5021 - val_loss: 5.6693\n",
            "Epoch 320/1000\n",
            "81/81 [==============================] - 0s 891us/step - loss: 26.7500 - val_loss: 3.0919\n",
            "Epoch 321/1000\n",
            "81/81 [==============================] - 0s 889us/step - loss: 36.5637 - val_loss: 8.4385\n",
            "Epoch 322/1000\n",
            "81/81 [==============================] - 0s 913us/step - loss: 26.7317 - val_loss: 2.2680\n",
            "Epoch 323/1000\n",
            "81/81 [==============================] - 0s 885us/step - loss: 31.9711 - val_loss: 3.1092\n",
            "Epoch 324/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 38.7441 - val_loss: 10.4320\n",
            "Epoch 325/1000\n",
            "81/81 [==============================] - 0s 859us/step - loss: 54.9697 - val_loss: 1.8192\n",
            "Epoch 326/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 34.9658 - val_loss: 6.7267\n",
            "Epoch 327/1000\n",
            "81/81 [==============================] - 0s 890us/step - loss: 43.1421 - val_loss: 2.3923\n",
            "Epoch 328/1000\n",
            "81/81 [==============================] - 0s 835us/step - loss: 32.0171 - val_loss: 5.4938\n",
            "Epoch 329/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 35.9745 - val_loss: 7.1894\n",
            "Epoch 330/1000\n",
            "81/81 [==============================] - 0s 894us/step - loss: 40.5360 - val_loss: 2.2128\n",
            "Epoch 331/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 33.3593 - val_loss: 5.9040\n",
            "Epoch 332/1000\n",
            "81/81 [==============================] - 0s 904us/step - loss: 39.3512 - val_loss: 4.9823\n",
            "Epoch 333/1000\n",
            "81/81 [==============================] - 0s 980us/step - loss: 29.7597 - val_loss: 2.1151\n",
            "Epoch 334/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 33.4772 - val_loss: 2.4228\n",
            "Epoch 335/1000\n",
            "81/81 [==============================] - 0s 827us/step - loss: 35.1669 - val_loss: 5.7171\n",
            "Epoch 336/1000\n",
            "81/81 [==============================] - 0s 869us/step - loss: 25.1996 - val_loss: 8.8109\n",
            "Epoch 337/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 49.5803 - val_loss: 8.3022\n",
            "Epoch 338/1000\n",
            "81/81 [==============================] - 0s 910us/step - loss: 33.8414 - val_loss: 4.5634\n",
            "Epoch 339/1000\n",
            "81/81 [==============================] - 0s 898us/step - loss: 27.3498 - val_loss: 3.7270\n",
            "Epoch 340/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 41.3528 - val_loss: 3.6523\n",
            "Epoch 341/1000\n",
            "81/81 [==============================] - 0s 828us/step - loss: 34.7683 - val_loss: 11.7276\n",
            "Epoch 342/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 39.2143 - val_loss: 4.9327\n",
            "Epoch 343/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 37.4351 - val_loss: 1.7842\n",
            "Epoch 344/1000\n",
            "81/81 [==============================] - 0s 885us/step - loss: 28.3751 - val_loss: 2.0841\n",
            "Epoch 345/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 32.2708 - val_loss: 6.0914\n",
            "Epoch 346/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 35.8362 - val_loss: 3.3913\n",
            "Epoch 347/1000\n",
            "81/81 [==============================] - 0s 917us/step - loss: 32.6204 - val_loss: 2.8712\n",
            "Epoch 348/1000\n",
            "81/81 [==============================] - 0s 958us/step - loss: 26.7698 - val_loss: 2.3413\n",
            "Epoch 349/1000\n",
            "81/81 [==============================] - 0s 887us/step - loss: 40.1831 - val_loss: 2.6117\n",
            "Epoch 350/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 30.0709 - val_loss: 6.5256\n",
            "Epoch 351/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 26.8873 - val_loss: 10.7070\n",
            "Epoch 352/1000\n",
            "81/81 [==============================] - 0s 959us/step - loss: 38.0475 - val_loss: 5.0595\n",
            "Epoch 353/1000\n",
            "81/81 [==============================] - 0s 872us/step - loss: 35.5400 - val_loss: 3.7102\n",
            "Epoch 354/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 35.3873 - val_loss: 4.5734\n",
            "Epoch 355/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 36.5579 - val_loss: 5.6935\n",
            "Epoch 356/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 31.6875 - val_loss: 3.8828\n",
            "Epoch 357/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 33.7454 - val_loss: 6.1250\n",
            "Epoch 358/1000\n",
            "81/81 [==============================] - 0s 902us/step - loss: 40.9489 - val_loss: 1.5726\n",
            "Epoch 359/1000\n",
            "81/81 [==============================] - 0s 904us/step - loss: 29.5059 - val_loss: 5.2134\n",
            "Epoch 360/1000\n",
            "81/81 [==============================] - 0s 884us/step - loss: 35.7307 - val_loss: 3.0658\n",
            "Epoch 361/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 35.1030 - val_loss: 1.5741\n",
            "Epoch 362/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 32.2972 - val_loss: 8.4063\n",
            "Epoch 363/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 40.1221 - val_loss: 2.8633\n",
            "Epoch 364/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 32.1063 - val_loss: 5.3061\n",
            "Epoch 365/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 32.0306 - val_loss: 6.5590\n",
            "Epoch 366/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 31.1042 - val_loss: 6.3870\n",
            "Epoch 367/1000\n",
            "81/81 [==============================] - 0s 819us/step - loss: 33.4706 - val_loss: 1.9044\n",
            "Epoch 368/1000\n",
            "81/81 [==============================] - 0s 827us/step - loss: 35.5914 - val_loss: 2.0558\n",
            "Epoch 369/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 43.1684 - val_loss: 5.4966\n",
            "Epoch 370/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 32.4418 - val_loss: 1.8116\n",
            "Epoch 371/1000\n",
            "81/81 [==============================] - 0s 900us/step - loss: 34.0508 - val_loss: 7.8077\n",
            "Epoch 372/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 31.4640 - val_loss: 3.6167\n",
            "Epoch 373/1000\n",
            "81/81 [==============================] - 0s 833us/step - loss: 29.3468 - val_loss: 3.0744\n",
            "Epoch 374/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 32.7579 - val_loss: 7.2232\n",
            "Epoch 375/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 35.4980 - val_loss: 3.1507\n",
            "Epoch 376/1000\n",
            "81/81 [==============================] - 0s 871us/step - loss: 36.3218 - val_loss: 2.6407\n",
            "Epoch 377/1000\n",
            "81/81 [==============================] - 0s 897us/step - loss: 31.9100 - val_loss: 6.4994\n",
            "Epoch 378/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 30.3925 - val_loss: 3.2256\n",
            "Epoch 379/1000\n",
            "81/81 [==============================] - 0s 826us/step - loss: 27.2353 - val_loss: 3.0618\n",
            "Epoch 380/1000\n",
            "81/81 [==============================] - 0s 877us/step - loss: 28.8313 - val_loss: 3.4360\n",
            "Epoch 381/1000\n",
            "81/81 [==============================] - 0s 811us/step - loss: 27.0931 - val_loss: 3.8956\n",
            "Epoch 382/1000\n",
            "81/81 [==============================] - 0s 826us/step - loss: 36.0412 - val_loss: 5.5228\n",
            "Epoch 383/1000\n",
            "81/81 [==============================] - 0s 885us/step - loss: 23.2523 - val_loss: 7.2454\n",
            "Epoch 384/1000\n",
            "81/81 [==============================] - 0s 836us/step - loss: 32.9455 - val_loss: 5.7189\n",
            "Epoch 385/1000\n",
            "81/81 [==============================] - 0s 822us/step - loss: 24.9968 - val_loss: 7.0409\n",
            "Epoch 386/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 30.6385 - val_loss: 3.1071\n",
            "Epoch 387/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 24.1121 - val_loss: 5.4036\n",
            "Epoch 388/1000\n",
            "81/81 [==============================] - 0s 910us/step - loss: 33.1930 - val_loss: 5.1223\n",
            "Epoch 389/1000\n",
            "81/81 [==============================] - 0s 888us/step - loss: 38.9097 - val_loss: 4.7185\n",
            "Epoch 390/1000\n",
            "81/81 [==============================] - 0s 984us/step - loss: 30.4705 - val_loss: 1.5025\n",
            "Epoch 391/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 32.2766 - val_loss: 1.8900\n",
            "Epoch 392/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 26.0688 - val_loss: 9.9077\n",
            "Epoch 393/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 28.5497 - val_loss: 5.0147\n",
            "Epoch 394/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 29.2708 - val_loss: 1.4770\n",
            "Epoch 395/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 41.8159 - val_loss: 9.2997\n",
            "Epoch 396/1000\n",
            "81/81 [==============================] - 0s 884us/step - loss: 38.2722 - val_loss: 2.0251\n",
            "Epoch 397/1000\n",
            "81/81 [==============================] - 0s 925us/step - loss: 22.9074 - val_loss: 2.6845\n",
            "Epoch 398/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 28.9351 - val_loss: 4.2833\n",
            "Epoch 399/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 37.7068 - val_loss: 2.7250\n",
            "Epoch 400/1000\n",
            "81/81 [==============================] - 0s 913us/step - loss: 28.8788 - val_loss: 3.9330\n",
            "Epoch 401/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 33.6124 - val_loss: 6.1697\n",
            "Epoch 402/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 35.2903 - val_loss: 2.5148\n",
            "Epoch 403/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 21.5047 - val_loss: 10.0127\n",
            "Epoch 404/1000\n",
            "81/81 [==============================] - 0s 959us/step - loss: 31.5016 - val_loss: 6.4634\n",
            "Epoch 405/1000\n",
            "81/81 [==============================] - 0s 886us/step - loss: 26.8668 - val_loss: 4.6270\n",
            "Epoch 406/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 31.1005 - val_loss: 3.5705\n",
            "Epoch 407/1000\n",
            "81/81 [==============================] - 0s 826us/step - loss: 22.4904 - val_loss: 5.6602\n",
            "Epoch 408/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 29.0573 - val_loss: 4.5723\n",
            "Epoch 409/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 18.1317 - val_loss: 9.2169\n",
            "Epoch 410/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 39.1421 - val_loss: 6.7036\n",
            "Epoch 411/1000\n",
            "81/81 [==============================] - 0s 900us/step - loss: 30.4034 - val_loss: 1.1595\n",
            "Epoch 412/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 40.3949 - val_loss: 5.1234\n",
            "Epoch 413/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 33.5618 - val_loss: 6.9916\n",
            "Epoch 414/1000\n",
            "81/81 [==============================] - 0s 859us/step - loss: 25.9839 - val_loss: 6.9081\n",
            "Epoch 415/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 30.4189 - val_loss: 7.4019\n",
            "Epoch 416/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 29.5181 - val_loss: 7.4529\n",
            "Epoch 417/1000\n",
            "81/81 [==============================] - 0s 891us/step - loss: 30.3858 - val_loss: 4.9323\n",
            "Epoch 418/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 33.7836 - val_loss: 5.5073\n",
            "Epoch 419/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 30.8023 - val_loss: 5.3321\n",
            "Epoch 420/1000\n",
            "81/81 [==============================] - 0s 878us/step - loss: 26.3849 - val_loss: 4.5364\n",
            "Epoch 421/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 31.4928 - val_loss: 5.5105\n",
            "Epoch 422/1000\n",
            "81/81 [==============================] - 0s 949us/step - loss: 21.4745 - val_loss: 5.5219\n",
            "Epoch 423/1000\n",
            "81/81 [==============================] - 0s 938us/step - loss: 36.3884 - val_loss: 4.7354\n",
            "Epoch 424/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 32.3810 - val_loss: 3.5256\n",
            "Epoch 425/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 23.0515 - val_loss: 3.0857\n",
            "Epoch 426/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 30.8439 - val_loss: 4.8544\n",
            "Epoch 427/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 29.4068 - val_loss: 3.6421\n",
            "Epoch 428/1000\n",
            "81/81 [==============================] - 0s 901us/step - loss: 27.6011 - val_loss: 4.4896\n",
            "Epoch 429/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 32.7901 - val_loss: 2.4068\n",
            "Epoch 430/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 18.7865 - val_loss: 2.3734\n",
            "Epoch 431/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 29.2267 - val_loss: 3.8649\n",
            "Epoch 432/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 23.5917 - val_loss: 4.8992\n",
            "Epoch 433/1000\n",
            "81/81 [==============================] - 0s 939us/step - loss: 34.3977 - val_loss: 5.6396\n",
            "Epoch 434/1000\n",
            "81/81 [==============================] - 0s 894us/step - loss: 42.3118 - val_loss: 4.9628\n",
            "Epoch 435/1000\n",
            "81/81 [==============================] - 0s 884us/step - loss: 27.3554 - val_loss: 3.2152\n",
            "Epoch 436/1000\n",
            "81/81 [==============================] - 0s 829us/step - loss: 22.8092 - val_loss: 4.4360\n",
            "Epoch 437/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 26.9412 - val_loss: 3.5419\n",
            "Epoch 438/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 27.6459 - val_loss: 1.8308\n",
            "Epoch 439/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 28.0052 - val_loss: 8.3188\n",
            "Epoch 440/1000\n",
            "81/81 [==============================] - 0s 891us/step - loss: 28.0118 - val_loss: 3.8584\n",
            "Epoch 441/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 29.1667 - val_loss: 5.7319\n",
            "Epoch 442/1000\n",
            "81/81 [==============================] - 0s 823us/step - loss: 41.9971 - val_loss: 6.1914\n",
            "Epoch 443/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 27.5474 - val_loss: 3.1420\n",
            "Epoch 444/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 33.9265 - val_loss: 3.5823\n",
            "Epoch 445/1000\n",
            "81/81 [==============================] - 0s 827us/step - loss: 24.6544 - val_loss: 1.4452\n",
            "Epoch 446/1000\n",
            "81/81 [==============================] - 0s 880us/step - loss: 27.3900 - val_loss: 7.2459\n",
            "Epoch 447/1000\n",
            "81/81 [==============================] - 0s 946us/step - loss: 46.5882 - val_loss: 2.2839\n",
            "Epoch 448/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 32.6041 - val_loss: 1.4980\n",
            "Epoch 449/1000\n",
            "81/81 [==============================] - 0s 836us/step - loss: 19.2992 - val_loss: 5.5068\n",
            "Epoch 450/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 23.5857 - val_loss: 7.1549\n",
            "Epoch 451/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 29.2300 - val_loss: 3.8116\n",
            "Epoch 452/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 28.5350 - val_loss: 3.8024\n",
            "Epoch 453/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 34.8265 - val_loss: 2.7005\n",
            "Epoch 454/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 31.3905 - val_loss: 8.5158\n",
            "Epoch 455/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 40.4302 - val_loss: 4.4246\n",
            "Epoch 456/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 23.8020 - val_loss: 9.4478\n",
            "Epoch 457/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 29.4513 - val_loss: 2.7064\n",
            "Epoch 458/1000\n",
            "81/81 [==============================] - 0s 887us/step - loss: 28.0096 - val_loss: 4.1953\n",
            "Epoch 459/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 27.7658 - val_loss: 7.9181\n",
            "Epoch 460/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 25.8735 - val_loss: 4.0003\n",
            "Epoch 461/1000\n",
            "81/81 [==============================] - 0s 896us/step - loss: 28.6943 - val_loss: 3.9114\n",
            "Epoch 462/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 28.4795 - val_loss: 2.1643\n",
            "Epoch 463/1000\n",
            "81/81 [==============================] - 0s 890us/step - loss: 26.3091 - val_loss: 1.8520\n",
            "Epoch 464/1000\n",
            "81/81 [==============================] - 0s 989us/step - loss: 29.8738 - val_loss: 3.9091\n",
            "Epoch 465/1000\n",
            "81/81 [==============================] - 0s 824us/step - loss: 34.7905 - val_loss: 4.5103\n",
            "Epoch 466/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 26.4003 - val_loss: 2.5602\n",
            "Epoch 467/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 25.8824 - val_loss: 6.5913\n",
            "Epoch 468/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 32.1834 - val_loss: 2.2035\n",
            "Epoch 469/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 28.0000 - val_loss: 6.3782\n",
            "Epoch 470/1000\n",
            "81/81 [==============================] - 0s 877us/step - loss: 31.7592 - val_loss: 1.2640\n",
            "Epoch 471/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 32.4880 - val_loss: 7.1233\n",
            "Epoch 472/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 31.6233 - val_loss: 3.5135\n",
            "Epoch 473/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 34.1864 - val_loss: 1.6421\n",
            "Epoch 474/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 29.8477 - val_loss: 9.2589\n",
            "Epoch 475/1000\n",
            "81/81 [==============================] - 0s 917us/step - loss: 22.1523 - val_loss: 3.0778\n",
            "Epoch 476/1000\n",
            "81/81 [==============================] - 0s 922us/step - loss: 18.1693 - val_loss: 3.8986\n",
            "Epoch 477/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 28.9176 - val_loss: 3.9787\n",
            "Epoch 478/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 25.4309 - val_loss: 4.3764\n",
            "Epoch 479/1000\n",
            "81/81 [==============================] - 0s 905us/step - loss: 22.1973 - val_loss: 3.8714\n",
            "Epoch 480/1000\n",
            "81/81 [==============================] - 0s 899us/step - loss: 39.6817 - val_loss: 4.6717\n",
            "Epoch 481/1000\n",
            "81/81 [==============================] - 0s 963us/step - loss: 23.7800 - val_loss: 3.1371\n",
            "Epoch 482/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 28.9088 - val_loss: 5.0647\n",
            "Epoch 483/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 31.5030 - val_loss: 1.7589\n",
            "Epoch 484/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 23.4661 - val_loss: 4.6270\n",
            "Epoch 485/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 24.7672 - val_loss: 3.9042\n",
            "Epoch 486/1000\n",
            "81/81 [==============================] - 0s 924us/step - loss: 19.9113 - val_loss: 1.8172\n",
            "Epoch 487/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 27.8932 - val_loss: 3.7385\n",
            "Epoch 488/1000\n",
            "81/81 [==============================] - 0s 859us/step - loss: 28.4890 - val_loss: 2.5865\n",
            "Epoch 489/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 29.1409 - val_loss: 6.7217\n",
            "Epoch 490/1000\n",
            "81/81 [==============================] - 0s 919us/step - loss: 25.6623 - val_loss: 1.8280\n",
            "Epoch 491/1000\n",
            "81/81 [==============================] - 0s 883us/step - loss: 25.2055 - val_loss: 2.9906\n",
            "Epoch 492/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 29.8929 - val_loss: 3.8647\n",
            "Epoch 493/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 24.3628 - val_loss: 7.3155\n",
            "Epoch 494/1000\n",
            "81/81 [==============================] - 0s 871us/step - loss: 27.6516 - val_loss: 4.1939\n",
            "Epoch 495/1000\n",
            "81/81 [==============================] - 0s 822us/step - loss: 26.6958 - val_loss: 3.7989\n",
            "Epoch 496/1000\n",
            "81/81 [==============================] - 0s 824us/step - loss: 28.5452 - val_loss: 3.2934\n",
            "Epoch 497/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 24.7047 - val_loss: 3.8120\n",
            "Epoch 498/1000\n",
            "81/81 [==============================] - 0s 970us/step - loss: 29.1963 - val_loss: 1.3836\n",
            "Epoch 499/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 31.7928 - val_loss: 10.0117\n",
            "Epoch 500/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 24.6544 - val_loss: 2.3838\n",
            "Epoch 501/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 34.2516 - val_loss: 1.6631\n",
            "Epoch 502/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 25.7593 - val_loss: 4.7779\n",
            "Epoch 503/1000\n",
            "81/81 [==============================] - 0s 895us/step - loss: 31.4579 - val_loss: 3.6157\n",
            "Epoch 504/1000\n",
            "81/81 [==============================] - 0s 889us/step - loss: 22.9055 - val_loss: 2.6402\n",
            "Epoch 505/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 23.2485 - val_loss: 3.5010\n",
            "Epoch 506/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 24.7273 - val_loss: 3.4342\n",
            "Epoch 507/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 28.3375 - val_loss: 1.8451\n",
            "Epoch 508/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 29.1973 - val_loss: 6.3607\n",
            "Epoch 509/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 23.6457 - val_loss: 2.7517\n",
            "Epoch 510/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 25.2269 - val_loss: 4.8254\n",
            "Epoch 511/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 24.4660 - val_loss: 3.8305\n",
            "Epoch 512/1000\n",
            "81/81 [==============================] - 0s 828us/step - loss: 27.7145 - val_loss: 3.2308\n",
            "Epoch 513/1000\n",
            "81/81 [==============================] - 0s 821us/step - loss: 26.5439 - val_loss: 4.3658\n",
            "Epoch 514/1000\n",
            "81/81 [==============================] - 0s 872us/step - loss: 30.3074 - val_loss: 6.2487\n",
            "Epoch 515/1000\n",
            "81/81 [==============================] - 0s 836us/step - loss: 19.0354 - val_loss: 3.1896\n",
            "Epoch 516/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 26.2839 - val_loss: 3.6505\n",
            "Epoch 517/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 29.7788 - val_loss: 2.7605\n",
            "Epoch 518/1000\n",
            "81/81 [==============================] - 0s 913us/step - loss: 23.3222 - val_loss: 2.0747\n",
            "Epoch 519/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 23.2155 - val_loss: 4.5157\n",
            "Epoch 520/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 28.5731 - val_loss: 2.5448\n",
            "Epoch 521/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 26.6132 - val_loss: 4.8130\n",
            "Epoch 522/1000\n",
            "81/81 [==============================] - 0s 897us/step - loss: 25.9713 - val_loss: 2.5352\n",
            "Epoch 523/1000\n",
            "81/81 [==============================] - 0s 874us/step - loss: 22.4641 - val_loss: 8.6439\n",
            "Epoch 524/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 29.4454 - val_loss: 2.5044\n",
            "Epoch 525/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 25.6101 - val_loss: 1.3797\n",
            "Epoch 526/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 27.1821 - val_loss: 7.8376\n",
            "Epoch 527/1000\n",
            "81/81 [==============================] - 0s 917us/step - loss: 23.7819 - val_loss: 2.1050\n",
            "Epoch 528/1000\n",
            "81/81 [==============================] - 0s 930us/step - loss: 24.8477 - val_loss: 6.7386\n",
            "Epoch 529/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 14.7221 - val_loss: 2.5257\n",
            "Epoch 530/1000\n",
            "81/81 [==============================] - 0s 870us/step - loss: 40.5450 - val_loss: 4.9994\n",
            "Epoch 531/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 23.0231 - val_loss: 2.1135\n",
            "Epoch 532/1000\n",
            "81/81 [==============================] - 0s 903us/step - loss: 28.6739 - val_loss: 1.5137\n",
            "Epoch 533/1000\n",
            "81/81 [==============================] - 0s 890us/step - loss: 23.2842 - val_loss: 1.5955\n",
            "Epoch 534/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 30.1178 - val_loss: 10.3598\n",
            "Epoch 535/1000\n",
            "81/81 [==============================] - 0s 828us/step - loss: 32.2477 - val_loss: 4.0869\n",
            "Epoch 536/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 20.4437 - val_loss: 3.5143\n",
            "Epoch 537/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 26.1706 - val_loss: 4.7834\n",
            "Epoch 538/1000\n",
            "81/81 [==============================] - 0s 835us/step - loss: 23.5515 - val_loss: 1.0661\n",
            "Epoch 539/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 32.3105 - val_loss: 9.0290\n",
            "Epoch 540/1000\n",
            "81/81 [==============================] - 0s 913us/step - loss: 22.5308 - val_loss: 1.6453\n",
            "Epoch 541/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 27.1628 - val_loss: 3.1205\n",
            "Epoch 542/1000\n",
            "81/81 [==============================] - 0s 886us/step - loss: 25.6654 - val_loss: 2.8433\n",
            "Epoch 543/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 21.4384 - val_loss: 1.2056\n",
            "Epoch 544/1000\n",
            "81/81 [==============================] - 0s 943us/step - loss: 24.5943 - val_loss: 6.5079\n",
            "Epoch 545/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 25.7484 - val_loss: 2.3369\n",
            "Epoch 546/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 16.6151 - val_loss: 3.7397\n",
            "Epoch 547/1000\n",
            "81/81 [==============================] - 0s 926us/step - loss: 27.5659 - val_loss: 6.9586\n",
            "Epoch 548/1000\n",
            "81/81 [==============================] - 0s 903us/step - loss: 19.4723 - val_loss: 1.6047\n",
            "Epoch 549/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 21.3207 - val_loss: 3.0514\n",
            "Epoch 550/1000\n",
            "81/81 [==============================] - 0s 877us/step - loss: 23.7805 - val_loss: 1.7353\n",
            "Epoch 551/1000\n",
            "81/81 [==============================] - 0s 827us/step - loss: 23.3358 - val_loss: 6.9830\n",
            "Epoch 552/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 26.1553 - val_loss: 1.4402\n",
            "Epoch 553/1000\n",
            "81/81 [==============================] - 0s 805us/step - loss: 24.6135 - val_loss: 3.3463\n",
            "Epoch 554/1000\n",
            "81/81 [==============================] - 0s 893us/step - loss: 21.3657 - val_loss: 2.4957\n",
            "Epoch 555/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 26.4239 - val_loss: 4.0882\n",
            "Epoch 556/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 17.6371 - val_loss: 6.5174\n",
            "Epoch 557/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 25.4941 - val_loss: 2.5828\n",
            "Epoch 558/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 24.5634 - val_loss: 4.8850\n",
            "Epoch 559/1000\n",
            "81/81 [==============================] - 0s 905us/step - loss: 27.7826 - val_loss: 1.2805\n",
            "Epoch 560/1000\n",
            "81/81 [==============================] - 0s 922us/step - loss: 22.0187 - val_loss: 3.4175\n",
            "Epoch 561/1000\n",
            "81/81 [==============================] - 0s 948us/step - loss: 26.7522 - val_loss: 2.3122\n",
            "Epoch 562/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 22.0357 - val_loss: 9.5603\n",
            "Epoch 563/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 22.3741 - val_loss: 4.7929\n",
            "Epoch 564/1000\n",
            "81/81 [==============================] - 0s 876us/step - loss: 25.6654 - val_loss: 3.1947\n",
            "Epoch 565/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 25.7362 - val_loss: 3.3860\n",
            "Epoch 566/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 20.7955 - val_loss: 2.2944\n",
            "Epoch 567/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 22.1733 - val_loss: 6.7941\n",
            "Epoch 568/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 23.3922 - val_loss: 2.3289\n",
            "Epoch 569/1000\n",
            "81/81 [==============================] - 0s 806us/step - loss: 31.6087 - val_loss: 1.4089\n",
            "Epoch 570/1000\n",
            "81/81 [==============================] - 0s 870us/step - loss: 21.6730 - val_loss: 13.3208\n",
            "Epoch 571/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 38.6362 - val_loss: 1.5479\n",
            "Epoch 572/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 22.9769 - val_loss: 2.6118\n",
            "Epoch 573/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 27.0090 - val_loss: 1.4244\n",
            "Epoch 574/1000\n",
            "81/81 [==============================] - 0s 829us/step - loss: 18.1979 - val_loss: 5.8052\n",
            "Epoch 575/1000\n",
            "81/81 [==============================] - 0s 899us/step - loss: 21.9217 - val_loss: 2.6788\n",
            "Epoch 576/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 27.6126 - val_loss: 6.7189\n",
            "Epoch 577/1000\n",
            "81/81 [==============================] - 0s 801us/step - loss: 17.6049 - val_loss: 6.7459\n",
            "Epoch 578/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 18.6518 - val_loss: 4.3087\n",
            "Epoch 579/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 28.3009 - val_loss: 1.2675\n",
            "Epoch 580/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 23.1878 - val_loss: 5.6146\n",
            "Epoch 581/1000\n",
            "81/81 [==============================] - 0s 824us/step - loss: 22.8623 - val_loss: 4.6186\n",
            "Epoch 582/1000\n",
            "81/81 [==============================] - 0s 820us/step - loss: 24.9055 - val_loss: 2.8410\n",
            "Epoch 583/1000\n",
            "81/81 [==============================] - 0s 885us/step - loss: 23.3731 - val_loss: 3.6091\n",
            "Epoch 584/1000\n",
            "81/81 [==============================] - 0s 889us/step - loss: 27.6646 - val_loss: 1.3148\n",
            "Epoch 585/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 23.9654 - val_loss: 4.8953\n",
            "Epoch 586/1000\n",
            "81/81 [==============================] - 0s 807us/step - loss: 23.2390 - val_loss: 3.0853\n",
            "Epoch 587/1000\n",
            "81/81 [==============================] - 0s 811us/step - loss: 23.6007 - val_loss: 4.4241\n",
            "Epoch 588/1000\n",
            "81/81 [==============================] - 0s 903us/step - loss: 27.6663 - val_loss: 2.4999\n",
            "Epoch 589/1000\n",
            "81/81 [==============================] - 0s 892us/step - loss: 17.1597 - val_loss: 2.8131\n",
            "Epoch 590/1000\n",
            "81/81 [==============================] - 0s 913us/step - loss: 18.7318 - val_loss: 2.0694\n",
            "Epoch 591/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 25.0075 - val_loss: 1.4158\n",
            "Epoch 592/1000\n",
            "81/81 [==============================] - 0s 894us/step - loss: 24.0997 - val_loss: 3.5332\n",
            "Epoch 593/1000\n",
            "81/81 [==============================] - 0s 836us/step - loss: 23.3529 - val_loss: 2.6949\n",
            "Epoch 594/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 21.0172 - val_loss: 2.4093\n",
            "Epoch 595/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 30.9064 - val_loss: 7.5985\n",
            "Epoch 596/1000\n",
            "81/81 [==============================] - 0s 828us/step - loss: 23.1781 - val_loss: 1.4462\n",
            "Epoch 597/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 25.0908 - val_loss: 7.2792\n",
            "Epoch 598/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 18.0575 - val_loss: 1.6214\n",
            "Epoch 599/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 25.1823 - val_loss: 9.3742\n",
            "Epoch 600/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 23.4809 - val_loss: 8.3245\n",
            "Epoch 601/1000\n",
            "81/81 [==============================] - 0s 921us/step - loss: 23.5296 - val_loss: 2.1479\n",
            "Epoch 602/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 26.8680 - val_loss: 4.4170\n",
            "Epoch 603/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 30.3242 - val_loss: 1.6249\n",
            "Epoch 604/1000\n",
            "81/81 [==============================] - 0s 992us/step - loss: 21.0747 - val_loss: 4.2183\n",
            "Epoch 605/1000\n",
            "81/81 [==============================] - 0s 874us/step - loss: 28.1388 - val_loss: 3.5884\n",
            "Epoch 606/1000\n",
            "81/81 [==============================] - 0s 871us/step - loss: 27.0093 - val_loss: 2.0665\n",
            "Epoch 607/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 28.7497 - val_loss: 3.9415\n",
            "Epoch 608/1000\n",
            "81/81 [==============================] - 0s 889us/step - loss: 21.2611 - val_loss: 1.3991\n",
            "Epoch 609/1000\n",
            "81/81 [==============================] - 0s 860us/step - loss: 34.2623 - val_loss: 4.8470\n",
            "Epoch 610/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 25.6811 - val_loss: 3.8344\n",
            "Epoch 611/1000\n",
            "81/81 [==============================] - 0s 884us/step - loss: 33.0425 - val_loss: 3.1698\n",
            "Epoch 612/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 20.9889 - val_loss: 1.2560\n",
            "Epoch 613/1000\n",
            "81/81 [==============================] - 0s 904us/step - loss: 24.4571 - val_loss: 5.2226\n",
            "Epoch 614/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 26.0434 - val_loss: 1.5083\n",
            "Epoch 615/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 24.6166 - val_loss: 5.5471\n",
            "Epoch 616/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 23.0058 - val_loss: 4.9375\n",
            "Epoch 617/1000\n",
            "81/81 [==============================] - 0s 836us/step - loss: 17.1097 - val_loss: 1.3304\n",
            "Epoch 618/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 26.2634 - val_loss: 4.7294\n",
            "Epoch 619/1000\n",
            "81/81 [==============================] - 0s 924us/step - loss: 24.4817 - val_loss: 2.4681\n",
            "Epoch 620/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 19.9449 - val_loss: 7.2496\n",
            "Epoch 621/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 16.3393 - val_loss: 1.2206\n",
            "Epoch 622/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 22.7789 - val_loss: 3.7253\n",
            "Epoch 623/1000\n",
            "81/81 [==============================] - 0s 872us/step - loss: 21.5835 - val_loss: 4.1119\n",
            "Epoch 624/1000\n",
            "81/81 [==============================] - 0s 941us/step - loss: 23.9367 - val_loss: 3.2333\n",
            "Epoch 625/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 23.8855 - val_loss: 3.0367\n",
            "Epoch 626/1000\n",
            "81/81 [==============================] - 0s 879us/step - loss: 25.8313 - val_loss: 2.8656\n",
            "Epoch 627/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 25.7774 - val_loss: 3.9164\n",
            "Epoch 628/1000\n",
            "81/81 [==============================] - 0s 821us/step - loss: 18.5293 - val_loss: 3.3133\n",
            "Epoch 629/1000\n",
            "81/81 [==============================] - 0s 818us/step - loss: 22.2365 - val_loss: 3.5839\n",
            "Epoch 630/1000\n",
            "81/81 [==============================] - 0s 895us/step - loss: 20.5384 - val_loss: 4.1266\n",
            "Epoch 631/1000\n",
            "81/81 [==============================] - 0s 813us/step - loss: 25.6618 - val_loss: 3.5655\n",
            "Epoch 632/1000\n",
            "81/81 [==============================] - 0s 833us/step - loss: 29.1135 - val_loss: 7.5269\n",
            "Epoch 633/1000\n",
            "81/81 [==============================] - 0s 892us/step - loss: 19.8082 - val_loss: 3.0951\n",
            "Epoch 634/1000\n",
            "81/81 [==============================] - 0s 820us/step - loss: 22.2553 - val_loss: 4.5602\n",
            "Epoch 635/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 25.5376 - val_loss: 4.6566\n",
            "Epoch 636/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 22.4067 - val_loss: 3.6115\n",
            "Epoch 637/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 20.5782 - val_loss: 6.0609\n",
            "Epoch 638/1000\n",
            "81/81 [==============================] - 0s 819us/step - loss: 23.2717 - val_loss: 5.3567\n",
            "Epoch 639/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 25.1727 - val_loss: 4.9362\n",
            "Epoch 640/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 23.2607 - val_loss: 3.2586\n",
            "Epoch 641/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 27.3808 - val_loss: 3.3925\n",
            "Epoch 642/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 24.3789 - val_loss: 2.9818\n",
            "Epoch 643/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 28.3198 - val_loss: 1.0281\n",
            "Epoch 644/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 23.9556 - val_loss: 2.4366\n",
            "Epoch 645/1000\n",
            "81/81 [==============================] - 0s 991us/step - loss: 22.2480 - val_loss: 3.7837\n",
            "Epoch 646/1000\n",
            "81/81 [==============================] - 0s 884us/step - loss: 23.0725 - val_loss: 0.9369\n",
            "Epoch 647/1000\n",
            "81/81 [==============================] - 0s 947us/step - loss: 23.3814 - val_loss: 2.9511\n",
            "Epoch 648/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 28.1949 - val_loss: 4.2839\n",
            "Epoch 649/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 22.8092 - val_loss: 2.2259\n",
            "Epoch 650/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 23.5775 - val_loss: 2.4558\n",
            "Epoch 651/1000\n",
            "81/81 [==============================] - 0s 869us/step - loss: 14.5716 - val_loss: 1.2704\n",
            "Epoch 652/1000\n",
            "81/81 [==============================] - 0s 904us/step - loss: 30.7119 - val_loss: 7.9516\n",
            "Epoch 653/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 13.2005 - val_loss: 2.2113\n",
            "Epoch 654/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 25.3059 - val_loss: 2.3937\n",
            "Epoch 655/1000\n",
            "81/81 [==============================] - 0s 881us/step - loss: 20.1347 - val_loss: 3.9575\n",
            "Epoch 656/1000\n",
            "81/81 [==============================] - 0s 887us/step - loss: 23.9850 - val_loss: 3.9285\n",
            "Epoch 657/1000\n",
            "81/81 [==============================] - 0s 882us/step - loss: 24.9959 - val_loss: 1.4332\n",
            "Epoch 658/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 21.4288 - val_loss: 2.2127\n",
            "Epoch 659/1000\n",
            "81/81 [==============================] - 0s 859us/step - loss: 15.7246 - val_loss: 1.6092\n",
            "Epoch 660/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 23.5778 - val_loss: 1.8664\n",
            "Epoch 661/1000\n",
            "81/81 [==============================] - 0s 887us/step - loss: 15.8544 - val_loss: 2.8564\n",
            "Epoch 662/1000\n",
            "81/81 [==============================] - 0s 932us/step - loss: 26.5932 - val_loss: 1.7368\n",
            "Epoch 663/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 20.5397 - val_loss: 1.6851\n",
            "Epoch 664/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 24.5148 - val_loss: 1.7949\n",
            "Epoch 665/1000\n",
            "81/81 [==============================] - 0s 825us/step - loss: 17.4794 - val_loss: 1.6304\n",
            "Epoch 666/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 26.7675 - val_loss: 4.0843\n",
            "Epoch 667/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 22.5732 - val_loss: 4.1602\n",
            "Epoch 668/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 23.9230 - val_loss: 5.1235\n",
            "Epoch 669/1000\n",
            "81/81 [==============================] - 0s 939us/step - loss: 19.6097 - val_loss: 2.8334\n",
            "Epoch 670/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 23.2217 - val_loss: 1.9579\n",
            "Epoch 671/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 26.8788 - val_loss: 1.4564\n",
            "Epoch 672/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 20.3731 - val_loss: 1.3949\n",
            "Epoch 673/1000\n",
            "81/81 [==============================] - 0s 903us/step - loss: 27.5619 - val_loss: 2.1527\n",
            "Epoch 674/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 20.9718 - val_loss: 1.3052\n",
            "Epoch 675/1000\n",
            "81/81 [==============================] - 0s 967us/step - loss: 22.1380 - val_loss: 1.3846\n",
            "Epoch 676/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 20.3973 - val_loss: 1.1910\n",
            "Epoch 677/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 17.7851 - val_loss: 9.0451\n",
            "Epoch 678/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 21.9535 - val_loss: 1.5811\n",
            "Epoch 679/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 26.0488 - val_loss: 5.0559\n",
            "Epoch 680/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 15.9623 - val_loss: 3.1289\n",
            "Epoch 681/1000\n",
            "81/81 [==============================] - 0s 892us/step - loss: 21.3703 - val_loss: 1.1698\n",
            "Epoch 682/1000\n",
            "81/81 [==============================] - 0s 859us/step - loss: 18.4397 - val_loss: 5.1767\n",
            "Epoch 683/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 21.7830 - val_loss: 3.8701\n",
            "Epoch 684/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 20.2344 - val_loss: 5.5150\n",
            "Epoch 685/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 21.0380 - val_loss: 3.9902\n",
            "Epoch 686/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 19.4501 - val_loss: 2.2976\n",
            "Epoch 687/1000\n",
            "81/81 [==============================] - 0s 878us/step - loss: 22.4707 - val_loss: 1.2783\n",
            "Epoch 688/1000\n",
            "81/81 [==============================] - 0s 915us/step - loss: 25.7899 - val_loss: 1.9627\n",
            "Epoch 689/1000\n",
            "81/81 [==============================] - 0s 890us/step - loss: 22.9797 - val_loss: 2.1589\n",
            "Epoch 690/1000\n",
            "81/81 [==============================] - 0s 883us/step - loss: 19.2067 - val_loss: 2.2186\n",
            "Epoch 691/1000\n",
            "81/81 [==============================] - 0s 876us/step - loss: 21.1360 - val_loss: 2.2562\n",
            "Epoch 692/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 26.1892 - val_loss: 5.9775\n",
            "Epoch 693/1000\n",
            "81/81 [==============================] - 0s 877us/step - loss: 16.7700 - val_loss: 1.8338\n",
            "Epoch 694/1000\n",
            "81/81 [==============================] - 0s 896us/step - loss: 20.0290 - val_loss: 2.0648\n",
            "Epoch 695/1000\n",
            "81/81 [==============================] - 0s 923us/step - loss: 18.2352 - val_loss: 3.5592\n",
            "Epoch 696/1000\n",
            "81/81 [==============================] - 0s 880us/step - loss: 18.9672 - val_loss: 1.4713\n",
            "Epoch 697/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 24.1218 - val_loss: 1.7240\n",
            "Epoch 698/1000\n",
            "81/81 [==============================] - 0s 833us/step - loss: 25.8460 - val_loss: 3.2517\n",
            "Epoch 699/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 22.2529 - val_loss: 2.7803\n",
            "Epoch 700/1000\n",
            "81/81 [==============================] - 0s 924us/step - loss: 22.6738 - val_loss: 2.8600\n",
            "Epoch 701/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 16.5714 - val_loss: 1.7385\n",
            "Epoch 702/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 21.0485 - val_loss: 6.1488\n",
            "Epoch 703/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 20.4964 - val_loss: 2.8862\n",
            "Epoch 704/1000\n",
            "81/81 [==============================] - 0s 928us/step - loss: 15.8548 - val_loss: 3.7559\n",
            "Epoch 705/1000\n",
            "81/81 [==============================] - 0s 902us/step - loss: 18.3620 - val_loss: 3.7222\n",
            "Epoch 706/1000\n",
            "81/81 [==============================] - 0s 907us/step - loss: 15.2064 - val_loss: 1.5883\n",
            "Epoch 707/1000\n",
            "81/81 [==============================] - 0s 836us/step - loss: 26.0043 - val_loss: 3.6653\n",
            "Epoch 708/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 18.6534 - val_loss: 4.9728\n",
            "Epoch 709/1000\n",
            "81/81 [==============================] - 0s 902us/step - loss: 21.1034 - val_loss: 1.6798\n",
            "Epoch 710/1000\n",
            "81/81 [==============================] - 0s 887us/step - loss: 28.2931 - val_loss: 4.1892\n",
            "Epoch 711/1000\n",
            "81/81 [==============================] - 0s 878us/step - loss: 19.9556 - val_loss: 3.0424\n",
            "Epoch 712/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 21.5698 - val_loss: 1.4608\n",
            "Epoch 713/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 19.0038 - val_loss: 5.7431\n",
            "Epoch 714/1000\n",
            "81/81 [==============================] - 0s 927us/step - loss: 21.5083 - val_loss: 5.7370\n",
            "Epoch 715/1000\n",
            "81/81 [==============================] - 0s 879us/step - loss: 19.8935 - val_loss: 1.5869\n",
            "Epoch 716/1000\n",
            "81/81 [==============================] - 0s 826us/step - loss: 20.6703 - val_loss: 3.6486\n",
            "Epoch 717/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 28.1423 - val_loss: 3.3358\n",
            "Epoch 718/1000\n",
            "81/81 [==============================] - 0s 883us/step - loss: 17.5403 - val_loss: 5.5269\n",
            "Epoch 719/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 17.1566 - val_loss: 3.5268\n",
            "Epoch 720/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 19.0729 - val_loss: 1.1991\n",
            "Epoch 721/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 18.0876 - val_loss: 8.0254\n",
            "Epoch 722/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 19.9733 - val_loss: 2.9872\n",
            "Epoch 723/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 20.4109 - val_loss: 1.4819\n",
            "Epoch 724/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 21.8903 - val_loss: 5.3414\n",
            "Epoch 725/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 20.1794 - val_loss: 1.8958\n",
            "Epoch 726/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 16.4165 - val_loss: 3.4001\n",
            "Epoch 727/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 19.0847 - val_loss: 1.6556\n",
            "Epoch 728/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 20.6261 - val_loss: 3.0731\n",
            "Epoch 729/1000\n",
            "81/81 [==============================] - 0s 991us/step - loss: 23.9627 - val_loss: 2.1761\n",
            "Epoch 730/1000\n",
            "81/81 [==============================] - 0s 878us/step - loss: 18.2181 - val_loss: 5.2905\n",
            "Epoch 731/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 20.3157 - val_loss: 2.1632\n",
            "Epoch 732/1000\n",
            "81/81 [==============================] - 0s 980us/step - loss: 22.8366 - val_loss: 1.9592\n",
            "Epoch 733/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 23.8746 - val_loss: 6.8738\n",
            "Epoch 734/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 18.5982 - val_loss: 2.0830\n",
            "Epoch 735/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 19.6426 - val_loss: 2.3852\n",
            "Epoch 736/1000\n",
            "81/81 [==============================] - 0s 987us/step - loss: 21.1170 - val_loss: 1.4852\n",
            "Epoch 737/1000\n",
            "81/81 [==============================] - 0s 876us/step - loss: 22.3027 - val_loss: 4.2821\n",
            "Epoch 738/1000\n",
            "81/81 [==============================] - 0s 840us/step - loss: 16.7579 - val_loss: 3.1208\n",
            "Epoch 739/1000\n",
            "81/81 [==============================] - 0s 819us/step - loss: 22.1501 - val_loss: 1.8644\n",
            "Epoch 740/1000\n",
            "81/81 [==============================] - 0s 871us/step - loss: 20.5592 - val_loss: 2.5290\n",
            "Epoch 741/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 18.0719 - val_loss: 2.3708\n",
            "Epoch 742/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 21.3838 - val_loss: 2.5636\n",
            "Epoch 743/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 23.2249 - val_loss: 1.9801\n",
            "Epoch 744/1000\n",
            "81/81 [==============================] - 0s 913us/step - loss: 28.2456 - val_loss: 2.4453\n",
            "Epoch 745/1000\n",
            "81/81 [==============================] - 0s 880us/step - loss: 25.5064 - val_loss: 4.9315\n",
            "Epoch 746/1000\n",
            "81/81 [==============================] - 0s 918us/step - loss: 21.3842 - val_loss: 3.9139\n",
            "Epoch 747/1000\n",
            "81/81 [==============================] - 0s 982us/step - loss: 17.8456 - val_loss: 2.0360\n",
            "Epoch 748/1000\n",
            "81/81 [==============================] - 0s 925us/step - loss: 24.5064 - val_loss: 3.4841\n",
            "Epoch 749/1000\n",
            "81/81 [==============================] - 0s 823us/step - loss: 20.6330 - val_loss: 0.9205\n",
            "Epoch 750/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 20.6946 - val_loss: 3.9935\n",
            "Epoch 751/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 19.6049 - val_loss: 2.7530\n",
            "Epoch 752/1000\n",
            "81/81 [==============================] - 0s 908us/step - loss: 19.2382 - val_loss: 2.7542\n",
            "Epoch 753/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 22.3062 - val_loss: 1.7961\n",
            "Epoch 754/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 17.4817 - val_loss: 3.2142\n",
            "Epoch 755/1000\n",
            "81/81 [==============================] - 0s 872us/step - loss: 22.1537 - val_loss: 1.5558\n",
            "Epoch 756/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 24.3110 - val_loss: 2.5764\n",
            "Epoch 757/1000\n",
            "81/81 [==============================] - 0s 926us/step - loss: 19.2103 - val_loss: 1.3425\n",
            "Epoch 758/1000\n",
            "81/81 [==============================] - 0s 917us/step - loss: 21.3700 - val_loss: 2.0563\n",
            "Epoch 759/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 19.2967 - val_loss: 4.9310\n",
            "Epoch 760/1000\n",
            "81/81 [==============================] - 0s 915us/step - loss: 25.0474 - val_loss: 2.1535\n",
            "Epoch 761/1000\n",
            "81/81 [==============================] - 0s 860us/step - loss: 21.4606 - val_loss: 3.7472\n",
            "Epoch 762/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 17.6874 - val_loss: 6.0918\n",
            "Epoch 763/1000\n",
            "81/81 [==============================] - 0s 874us/step - loss: 16.9533 - val_loss: 2.6297\n",
            "Epoch 764/1000\n",
            "81/81 [==============================] - 0s 925us/step - loss: 17.5555 - val_loss: 1.1005\n",
            "Epoch 765/1000\n",
            "81/81 [==============================] - 0s 937us/step - loss: 19.2943 - val_loss: 2.3193\n",
            "Epoch 766/1000\n",
            "81/81 [==============================] - 0s 901us/step - loss: 19.4151 - val_loss: 1.6785\n",
            "Epoch 767/1000\n",
            "81/81 [==============================] - 0s 819us/step - loss: 13.8292 - val_loss: 1.8323\n",
            "Epoch 768/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 20.9371 - val_loss: 1.5311\n",
            "Epoch 769/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 16.9005 - val_loss: 2.5556\n",
            "Epoch 770/1000\n",
            "81/81 [==============================] - 0s 973us/step - loss: 19.7421 - val_loss: 2.6704\n",
            "Epoch 771/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 17.7528 - val_loss: 2.3032\n",
            "Epoch 772/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 17.2916 - val_loss: 1.6375\n",
            "Epoch 773/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 19.7590 - val_loss: 2.4633\n",
            "Epoch 774/1000\n",
            "81/81 [==============================] - 0s 949us/step - loss: 18.1948 - val_loss: 2.4012\n",
            "Epoch 775/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 24.3419 - val_loss: 3.7156\n",
            "Epoch 776/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 24.9517 - val_loss: 2.4843\n",
            "Epoch 777/1000\n",
            "81/81 [==============================] - 0s 820us/step - loss: 15.4475 - val_loss: 2.2351\n",
            "Epoch 778/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 15.9874 - val_loss: 3.3520\n",
            "Epoch 779/1000\n",
            "81/81 [==============================] - 0s 890us/step - loss: 15.1468 - val_loss: 3.1707\n",
            "Epoch 780/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 18.7523 - val_loss: 1.8624\n",
            "Epoch 781/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 22.5033 - val_loss: 1.6677\n",
            "Epoch 782/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 24.9841 - val_loss: 1.3064\n",
            "Epoch 783/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 22.1460 - val_loss: 1.8531\n",
            "Epoch 784/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 25.2084 - val_loss: 1.8055\n",
            "Epoch 785/1000\n",
            "81/81 [==============================] - 0s 882us/step - loss: 24.4250 - val_loss: 2.0155\n",
            "Epoch 786/1000\n",
            "81/81 [==============================] - 0s 823us/step - loss: 15.6618 - val_loss: 3.3282\n",
            "Epoch 787/1000\n",
            "81/81 [==============================] - 0s 895us/step - loss: 16.4136 - val_loss: 2.8821\n",
            "Epoch 788/1000\n",
            "81/81 [==============================] - 0s 904us/step - loss: 20.9882 - val_loss: 1.9041\n",
            "Epoch 789/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 16.3577 - val_loss: 3.2684\n",
            "Epoch 790/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 18.6736 - val_loss: 4.6913\n",
            "Epoch 791/1000\n",
            "81/81 [==============================] - 0s 971us/step - loss: 21.7695 - val_loss: 1.7198\n",
            "Epoch 792/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 19.6752 - val_loss: 4.2933\n",
            "Epoch 793/1000\n",
            "81/81 [==============================] - 0s 869us/step - loss: 19.6639 - val_loss: 3.8622\n",
            "Epoch 794/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 14.5343 - val_loss: 2.8722\n",
            "Epoch 795/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 18.0088 - val_loss: 1.6544\n",
            "Epoch 796/1000\n",
            "81/81 [==============================] - 0s 860us/step - loss: 22.2778 - val_loss: 6.4152\n",
            "Epoch 797/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 24.0108 - val_loss: 1.4243\n",
            "Epoch 798/1000\n",
            "81/81 [==============================] - 0s 826us/step - loss: 18.1475 - val_loss: 1.7220\n",
            "Epoch 799/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 16.4514 - val_loss: 2.2183\n",
            "Epoch 800/1000\n",
            "81/81 [==============================] - 0s 920us/step - loss: 22.2595 - val_loss: 3.7197\n",
            "Epoch 801/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 19.7937 - val_loss: 1.2267\n",
            "Epoch 802/1000\n",
            "81/81 [==============================] - 0s 905us/step - loss: 18.2282 - val_loss: 2.3462\n",
            "Epoch 803/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 18.4399 - val_loss: 1.2803\n",
            "Epoch 804/1000\n",
            "81/81 [==============================] - 0s 860us/step - loss: 20.1191 - val_loss: 3.1324\n",
            "Epoch 805/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 22.4887 - val_loss: 1.5915\n",
            "Epoch 806/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 19.4829 - val_loss: 2.9637\n",
            "Epoch 807/1000\n",
            "81/81 [==============================] - 0s 905us/step - loss: 20.8861 - val_loss: 0.9555\n",
            "Epoch 808/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 22.1430 - val_loss: 5.3862\n",
            "Epoch 809/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 21.8698 - val_loss: 2.1601\n",
            "Epoch 810/1000\n",
            "81/81 [==============================] - 0s 869us/step - loss: 16.0739 - val_loss: 2.9356\n",
            "Epoch 811/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 17.3067 - val_loss: 3.8253\n",
            "Epoch 812/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 13.1046 - val_loss: 1.7644\n",
            "Epoch 813/1000\n",
            "81/81 [==============================] - 0s 935us/step - loss: 24.2033 - val_loss: 2.5688\n",
            "Epoch 814/1000\n",
            "81/81 [==============================] - 0s 901us/step - loss: 20.1153 - val_loss: 1.7397\n",
            "Epoch 815/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 15.4024 - val_loss: 1.2308\n",
            "Epoch 816/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 13.2920 - val_loss: 3.7620\n",
            "Epoch 817/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 22.8357 - val_loss: 1.0317\n",
            "Epoch 818/1000\n",
            "81/81 [==============================] - 0s 848us/step - loss: 21.6749 - val_loss: 2.2185\n",
            "Epoch 819/1000\n",
            "81/81 [==============================] - 0s 870us/step - loss: 15.6306 - val_loss: 2.6884\n",
            "Epoch 820/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 15.7101 - val_loss: 2.3113\n",
            "Epoch 821/1000\n",
            "81/81 [==============================] - 0s 882us/step - loss: 16.7047 - val_loss: 1.4499\n",
            "Epoch 822/1000\n",
            "81/81 [==============================] - 0s 826us/step - loss: 20.6588 - val_loss: 1.9194\n",
            "Epoch 823/1000\n",
            "81/81 [==============================] - 0s 871us/step - loss: 24.5760 - val_loss: 5.7015\n",
            "Epoch 824/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 16.0446 - val_loss: 2.5586\n",
            "Epoch 825/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 24.7579 - val_loss: 2.8789\n",
            "Epoch 826/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 19.1993 - val_loss: 3.0146\n",
            "Epoch 827/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 17.3866 - val_loss: 1.9123\n",
            "Epoch 828/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 18.1450 - val_loss: 5.1040\n",
            "Epoch 829/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 11.4170 - val_loss: 4.5004\n",
            "Epoch 830/1000\n",
            "81/81 [==============================] - 0s 931us/step - loss: 14.8272 - val_loss: 2.4449\n",
            "Epoch 831/1000\n",
            "81/81 [==============================] - 0s 891us/step - loss: 17.2522 - val_loss: 5.5499\n",
            "Epoch 832/1000\n",
            "81/81 [==============================] - 0s 867us/step - loss: 17.3455 - val_loss: 2.9175\n",
            "Epoch 833/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 19.5239 - val_loss: 2.2705\n",
            "Epoch 834/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 15.8855 - val_loss: 2.7479\n",
            "Epoch 835/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 13.3943 - val_loss: 1.1898\n",
            "Epoch 836/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 14.0517 - val_loss: 1.8324\n",
            "Epoch 837/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 17.9898 - val_loss: 1.4887\n",
            "Epoch 838/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 17.1871 - val_loss: 3.1867\n",
            "Epoch 839/1000\n",
            "81/81 [==============================] - 0s 945us/step - loss: 19.2241 - val_loss: 2.1525\n",
            "Epoch 840/1000\n",
            "81/81 [==============================] - 0s 910us/step - loss: 17.0873 - val_loss: 2.2543\n",
            "Epoch 841/1000\n",
            "81/81 [==============================] - 0s 837us/step - loss: 20.0954 - val_loss: 1.2785\n",
            "Epoch 842/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 14.4920 - val_loss: 2.0469\n",
            "Epoch 843/1000\n",
            "81/81 [==============================] - 0s 883us/step - loss: 19.8290 - val_loss: 2.0210\n",
            "Epoch 844/1000\n",
            "81/81 [==============================] - 0s 996us/step - loss: 21.1524 - val_loss: 2.7698\n",
            "Epoch 845/1000\n",
            "81/81 [==============================] - 0s 900us/step - loss: 15.4748 - val_loss: 2.8110\n",
            "Epoch 846/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 19.9867 - val_loss: 2.4951\n",
            "Epoch 847/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 16.2718 - val_loss: 1.0819\n",
            "Epoch 848/1000\n",
            "81/81 [==============================] - 0s 833us/step - loss: 23.0462 - val_loss: 1.7185\n",
            "Epoch 849/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 13.2787 - val_loss: 2.6947\n",
            "Epoch 850/1000\n",
            "81/81 [==============================] - 0s 909us/step - loss: 18.4064 - val_loss: 2.7100\n",
            "Epoch 851/1000\n",
            "81/81 [==============================] - 0s 925us/step - loss: 15.9250 - val_loss: 2.2278\n",
            "Epoch 852/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 20.6241 - val_loss: 2.3159\n",
            "Epoch 853/1000\n",
            "81/81 [==============================] - 0s 835us/step - loss: 15.3735 - val_loss: 1.2090\n",
            "Epoch 854/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 18.2233 - val_loss: 1.4412\n",
            "Epoch 855/1000\n",
            "81/81 [==============================] - 0s 874us/step - loss: 17.5117 - val_loss: 2.6743\n",
            "Epoch 856/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 11.8212 - val_loss: 3.5032\n",
            "Epoch 857/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 20.2650 - val_loss: 4.0219\n",
            "Epoch 858/1000\n",
            "81/81 [==============================] - 0s 926us/step - loss: 19.9309 - val_loss: 2.0727\n",
            "Epoch 859/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 13.4589 - val_loss: 5.7136\n",
            "Epoch 860/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 18.8636 - val_loss: 1.5369\n",
            "Epoch 861/1000\n",
            "81/81 [==============================] - 0s 891us/step - loss: 14.0753 - val_loss: 2.5143\n",
            "Epoch 862/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 23.7339 - val_loss: 1.0241\n",
            "Epoch 863/1000\n",
            "81/81 [==============================] - 0s 869us/step - loss: 15.9169 - val_loss: 3.1960\n",
            "Epoch 864/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 11.9780 - val_loss: 1.5227\n",
            "Epoch 865/1000\n",
            "81/81 [==============================] - 0s 903us/step - loss: 17.9030 - val_loss: 1.4358\n",
            "Epoch 866/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 21.7280 - val_loss: 1.3604\n",
            "Epoch 867/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 16.1732 - val_loss: 2.1509\n",
            "Epoch 868/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 18.0191 - val_loss: 2.0364\n",
            "Epoch 869/1000\n",
            "81/81 [==============================] - 0s 875us/step - loss: 19.4521 - val_loss: 2.3957\n",
            "Epoch 870/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 17.2585 - val_loss: 1.8710\n",
            "Epoch 871/1000\n",
            "81/81 [==============================] - 0s 938us/step - loss: 17.6567 - val_loss: 1.9795\n",
            "Epoch 872/1000\n",
            "81/81 [==============================] - 0s 935us/step - loss: 20.5079 - val_loss: 1.5449\n",
            "Epoch 873/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 14.3908 - val_loss: 3.4994\n",
            "Epoch 874/1000\n",
            "81/81 [==============================] - 0s 875us/step - loss: 18.7742 - val_loss: 1.8701\n",
            "Epoch 875/1000\n",
            "81/81 [==============================] - 0s 871us/step - loss: 13.2123 - val_loss: 1.8241\n",
            "Epoch 876/1000\n",
            "81/81 [==============================] - 0s 843us/step - loss: 25.5803 - val_loss: 3.2757\n",
            "Epoch 877/1000\n",
            "81/81 [==============================] - 0s 860us/step - loss: 22.5325 - val_loss: 3.9238\n",
            "Epoch 878/1000\n",
            "81/81 [==============================] - 0s 922us/step - loss: 14.6586 - val_loss: 1.4273\n",
            "Epoch 879/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 17.1148 - val_loss: 5.5044\n",
            "Epoch 880/1000\n",
            "81/81 [==============================] - 0s 863us/step - loss: 20.9789 - val_loss: 2.3394\n",
            "Epoch 881/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 15.6125 - val_loss: 2.7759\n",
            "Epoch 882/1000\n",
            "81/81 [==============================] - 0s 860us/step - loss: 17.6097 - val_loss: 2.5038\n",
            "Epoch 883/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 20.4755 - val_loss: 1.5527\n",
            "Epoch 884/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 17.9847 - val_loss: 1.9422\n",
            "Epoch 885/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 19.9983 - val_loss: 2.5132\n",
            "Epoch 886/1000\n",
            "81/81 [==============================] - 0s 910us/step - loss: 18.2794 - val_loss: 3.0165\n",
            "Epoch 887/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 17.0841 - val_loss: 2.2511\n",
            "Epoch 888/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 15.8510 - val_loss: 2.5650\n",
            "Epoch 889/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 11.7375 - val_loss: 2.7743\n",
            "Epoch 890/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 13.5389 - val_loss: 2.0806\n",
            "Epoch 891/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 13.8754 - val_loss: 1.4451\n",
            "Epoch 892/1000\n",
            "81/81 [==============================] - 0s 864us/step - loss: 17.0757 - val_loss: 2.6240\n",
            "Epoch 893/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 15.0962 - val_loss: 2.3469\n",
            "Epoch 894/1000\n",
            "81/81 [==============================] - 0s 874us/step - loss: 16.7108 - val_loss: 2.1813\n",
            "Epoch 895/1000\n",
            "81/81 [==============================] - 0s 963us/step - loss: 17.8997 - val_loss: 1.7075\n",
            "Epoch 896/1000\n",
            "81/81 [==============================] - 0s 903us/step - loss: 21.6073 - val_loss: 2.9556\n",
            "Epoch 897/1000\n",
            "81/81 [==============================] - 0s 870us/step - loss: 16.3965 - val_loss: 3.0403\n",
            "Epoch 898/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 18.8444 - val_loss: 2.4059\n",
            "Epoch 899/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 24.2383 - val_loss: 3.9626\n",
            "Epoch 900/1000\n",
            "81/81 [==============================] - 0s 943us/step - loss: 15.0384 - val_loss: 1.1603\n",
            "Epoch 901/1000\n",
            "81/81 [==============================] - 0s 912us/step - loss: 16.9794 - val_loss: 3.4782\n",
            "Epoch 902/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 15.7327 - val_loss: 1.8021\n",
            "Epoch 903/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 18.7024 - val_loss: 1.9406\n",
            "Epoch 904/1000\n",
            "81/81 [==============================] - 0s 871us/step - loss: 17.3229 - val_loss: 1.8802\n",
            "Epoch 905/1000\n",
            "81/81 [==============================] - 0s 932us/step - loss: 14.2231 - val_loss: 1.5303\n",
            "Epoch 906/1000\n",
            "81/81 [==============================] - 0s 820us/step - loss: 16.2768 - val_loss: 3.0678\n",
            "Epoch 907/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 14.3442 - val_loss: 1.0426\n",
            "Epoch 908/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 16.7921 - val_loss: 1.9515\n",
            "Epoch 909/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 15.1902 - val_loss: 2.8847\n",
            "Epoch 910/1000\n",
            "81/81 [==============================] - 0s 818us/step - loss: 15.1319 - val_loss: 2.0636\n",
            "Epoch 911/1000\n",
            "81/81 [==============================] - 0s 838us/step - loss: 15.9555 - val_loss: 1.9611\n",
            "Epoch 912/1000\n",
            "81/81 [==============================] - 0s 845us/step - loss: 14.8590 - val_loss: 1.8902\n",
            "Epoch 913/1000\n",
            "81/81 [==============================] - 0s 879us/step - loss: 13.4103 - val_loss: 1.2844\n",
            "Epoch 914/1000\n",
            "81/81 [==============================] - 0s 852us/step - loss: 19.6530 - val_loss: 3.5996\n",
            "Epoch 915/1000\n",
            "81/81 [==============================] - 0s 910us/step - loss: 22.4615 - val_loss: 1.3217\n",
            "Epoch 916/1000\n",
            "81/81 [==============================] - 0s 822us/step - loss: 18.3504 - val_loss: 1.4694\n",
            "Epoch 917/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 22.4952 - val_loss: 2.2611\n",
            "Epoch 918/1000\n",
            "81/81 [==============================] - 0s 875us/step - loss: 25.2932 - val_loss: 2.8748\n",
            "Epoch 919/1000\n",
            "81/81 [==============================] - 0s 849us/step - loss: 14.4592 - val_loss: 1.4050\n",
            "Epoch 920/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 15.0869 - val_loss: 3.9414\n",
            "Epoch 921/1000\n",
            "81/81 [==============================] - 0s 883us/step - loss: 25.5022 - val_loss: 1.9931\n",
            "Epoch 922/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 17.2783 - val_loss: 2.1265\n",
            "Epoch 923/1000\n",
            "81/81 [==============================] - 0s 822us/step - loss: 22.2586 - val_loss: 2.1242\n",
            "Epoch 924/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 21.2065 - val_loss: 2.5604\n",
            "Epoch 925/1000\n",
            "81/81 [==============================] - 0s 851us/step - loss: 12.9047 - val_loss: 3.4935\n",
            "Epoch 926/1000\n",
            "81/81 [==============================] - 0s 820us/step - loss: 18.5785 - val_loss: 2.8763\n",
            "Epoch 927/1000\n",
            "81/81 [==============================] - 0s 816us/step - loss: 18.1928 - val_loss: 2.1916\n",
            "Epoch 928/1000\n",
            "81/81 [==============================] - 0s 965us/step - loss: 15.0920 - val_loss: 1.5680\n",
            "Epoch 929/1000\n",
            "81/81 [==============================] - 0s 956us/step - loss: 18.3170 - val_loss: 2.0408\n",
            "Epoch 930/1000\n",
            "81/81 [==============================] - 0s 835us/step - loss: 16.6591 - val_loss: 1.4687\n",
            "Epoch 931/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 16.6393 - val_loss: 4.0091\n",
            "Epoch 932/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 18.7003 - val_loss: 2.4235\n",
            "Epoch 933/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 14.9542 - val_loss: 1.6814\n",
            "Epoch 934/1000\n",
            "81/81 [==============================] - 0s 937us/step - loss: 20.7212 - val_loss: 2.6713\n",
            "Epoch 935/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 15.0064 - val_loss: 3.1717\n",
            "Epoch 936/1000\n",
            "81/81 [==============================] - 0s 819us/step - loss: 17.2335 - val_loss: 2.4785\n",
            "Epoch 937/1000\n",
            "81/81 [==============================] - 0s 836us/step - loss: 16.1462 - val_loss: 3.3044\n",
            "Epoch 938/1000\n",
            "81/81 [==============================] - 0s 815us/step - loss: 12.3290 - val_loss: 1.4787\n",
            "Epoch 939/1000\n",
            "81/81 [==============================] - 0s 889us/step - loss: 17.6517 - val_loss: 1.5169\n",
            "Epoch 940/1000\n",
            "81/81 [==============================] - 0s 833us/step - loss: 16.7789 - val_loss: 3.0342\n",
            "Epoch 941/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 17.8693 - val_loss: 2.2350\n",
            "Epoch 942/1000\n",
            "81/81 [==============================] - 0s 831us/step - loss: 18.4712 - val_loss: 1.0691\n",
            "Epoch 943/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 15.8968 - val_loss: 2.5107\n",
            "Epoch 944/1000\n",
            "81/81 [==============================] - 0s 903us/step - loss: 19.7671 - val_loss: 2.6595\n",
            "Epoch 945/1000\n",
            "81/81 [==============================] - 0s 890us/step - loss: 18.4936 - val_loss: 2.9392\n",
            "Epoch 946/1000\n",
            "81/81 [==============================] - 0s 834us/step - loss: 18.3763 - val_loss: 1.9867\n",
            "Epoch 947/1000\n",
            "81/81 [==============================] - 0s 862us/step - loss: 16.0464 - val_loss: 1.5040\n",
            "Epoch 948/1000\n",
            "81/81 [==============================] - 0s 856us/step - loss: 16.4033 - val_loss: 2.6868\n",
            "Epoch 949/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 14.1747 - val_loss: 1.3788\n",
            "Epoch 950/1000\n",
            "81/81 [==============================] - 0s 914us/step - loss: 20.5102 - val_loss: 2.3490\n",
            "Epoch 951/1000\n",
            "81/81 [==============================] - 0s 897us/step - loss: 13.3715 - val_loss: 2.2809\n",
            "Epoch 952/1000\n",
            "81/81 [==============================] - 0s 858us/step - loss: 18.2885 - val_loss: 1.8073\n",
            "Epoch 953/1000\n",
            "81/81 [==============================] - 0s 876us/step - loss: 16.6137 - val_loss: 1.9502\n",
            "Epoch 954/1000\n",
            "81/81 [==============================] - 0s 844us/step - loss: 14.2359 - val_loss: 1.9683\n",
            "Epoch 955/1000\n",
            "81/81 [==============================] - 0s 859us/step - loss: 13.2415 - val_loss: 2.4184\n",
            "Epoch 956/1000\n",
            "81/81 [==============================] - 0s 951us/step - loss: 12.7012 - val_loss: 1.1625\n",
            "Epoch 957/1000\n",
            "81/81 [==============================] - 0s 884us/step - loss: 19.0261 - val_loss: 1.4552\n",
            "Epoch 958/1000\n",
            "81/81 [==============================] - 0s 898us/step - loss: 17.1702 - val_loss: 3.0869\n",
            "Epoch 959/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 13.7453 - val_loss: 1.3916\n",
            "Epoch 960/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 17.1105 - val_loss: 3.4366\n",
            "Epoch 961/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 18.9999 - val_loss: 1.1107\n",
            "Epoch 962/1000\n",
            "81/81 [==============================] - 0s 882us/step - loss: 15.3815 - val_loss: 1.1162\n",
            "Epoch 963/1000\n",
            "81/81 [==============================] - 0s 857us/step - loss: 13.7805 - val_loss: 3.1520\n",
            "Epoch 964/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 11.9022 - val_loss: 1.8596\n",
            "Epoch 965/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 10.4666 - val_loss: 2.2315\n",
            "Epoch 966/1000\n",
            "81/81 [==============================] - 0s 829us/step - loss: 14.3829 - val_loss: 2.7808\n",
            "Epoch 967/1000\n",
            "81/81 [==============================] - 0s 881us/step - loss: 18.5301 - val_loss: 1.7589\n",
            "Epoch 968/1000\n",
            "81/81 [==============================] - 0s 803us/step - loss: 14.2018 - val_loss: 3.3115\n",
            "Epoch 969/1000\n",
            "81/81 [==============================] - 0s 847us/step - loss: 13.4118 - val_loss: 1.2289\n",
            "Epoch 970/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 21.2716 - val_loss: 4.3144\n",
            "Epoch 971/1000\n",
            "81/81 [==============================] - 0s 832us/step - loss: 16.0251 - val_loss: 1.1739\n",
            "Epoch 972/1000\n",
            "81/81 [==============================] - 0s 931us/step - loss: 17.9201 - val_loss: 1.6926\n",
            "Epoch 973/1000\n",
            "81/81 [==============================] - 0s 889us/step - loss: 18.1987 - val_loss: 4.2120\n",
            "Epoch 974/1000\n",
            "81/81 [==============================] - 0s 839us/step - loss: 19.8840 - val_loss: 2.2404\n",
            "Epoch 975/1000\n",
            "81/81 [==============================] - 0s 854us/step - loss: 10.2798 - val_loss: 2.7705\n",
            "Epoch 976/1000\n",
            "81/81 [==============================] - 0s 841us/step - loss: 15.7017 - val_loss: 2.4597\n",
            "Epoch 977/1000\n",
            "81/81 [==============================] - 0s 865us/step - loss: 17.3177 - val_loss: 2.7147\n",
            "Epoch 978/1000\n",
            "81/81 [==============================] - 0s 880us/step - loss: 15.4839 - val_loss: 2.2347\n",
            "Epoch 979/1000\n",
            "81/81 [==============================] - 0s 912us/step - loss: 16.1873 - val_loss: 1.4978\n",
            "Epoch 980/1000\n",
            "81/81 [==============================] - 0s 866us/step - loss: 15.3742 - val_loss: 1.7375\n",
            "Epoch 981/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 12.0129 - val_loss: 3.0016\n",
            "Epoch 982/1000\n",
            "81/81 [==============================] - 0s 869us/step - loss: 12.3205 - val_loss: 3.6352\n",
            "Epoch 983/1000\n",
            "81/81 [==============================] - 0s 850us/step - loss: 18.9296 - val_loss: 1.3839\n",
            "Epoch 984/1000\n",
            "81/81 [==============================] - 0s 898us/step - loss: 17.2883 - val_loss: 1.4358\n",
            "Epoch 985/1000\n",
            "81/81 [==============================] - 0s 873us/step - loss: 12.8623 - val_loss: 2.3841\n",
            "Epoch 986/1000\n",
            "81/81 [==============================] - 0s 977us/step - loss: 12.8323 - val_loss: 1.5789\n",
            "Epoch 987/1000\n",
            "81/81 [==============================] - 0s 859us/step - loss: 16.2136 - val_loss: 1.2910\n",
            "Epoch 988/1000\n",
            "81/81 [==============================] - 0s 833us/step - loss: 15.3472 - val_loss: 1.5358\n",
            "Epoch 989/1000\n",
            "81/81 [==============================] - 0s 871us/step - loss: 16.6863 - val_loss: 1.3138\n",
            "Epoch 990/1000\n",
            "81/81 [==============================] - 0s 868us/step - loss: 17.7344 - val_loss: 1.5606\n",
            "Epoch 991/1000\n",
            "81/81 [==============================] - 0s 830us/step - loss: 12.2085 - val_loss: 2.4244\n",
            "Epoch 992/1000\n",
            "81/81 [==============================] - 0s 853us/step - loss: 15.6740 - val_loss: 2.3926\n",
            "Epoch 993/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 15.6190 - val_loss: 1.4480\n",
            "Epoch 994/1000\n",
            "81/81 [==============================] - 0s 855us/step - loss: 15.6311 - val_loss: 1.9872\n",
            "Epoch 995/1000\n",
            "81/81 [==============================] - 0s 901us/step - loss: 14.9755 - val_loss: 2.0480\n",
            "Epoch 996/1000\n",
            "81/81 [==============================] - 0s 915us/step - loss: 14.4508 - val_loss: 2.1786\n",
            "Epoch 997/1000\n",
            "81/81 [==============================] - 0s 842us/step - loss: 14.6163 - val_loss: 1.5374\n",
            "Epoch 998/1000\n",
            "81/81 [==============================] - 0s 820us/step - loss: 16.3799 - val_loss: 2.0275\n",
            "Epoch 999/1000\n",
            "81/81 [==============================] - 0s 846us/step - loss: 18.1139 - val_loss: 1.9277\n",
            "Epoch 1000/1000\n",
            "81/81 [==============================] - 0s 861us/step - loss: 15.6634 - val_loss: 3.1082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2387347da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5HdwjWTpSR3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "06a12ae6-ea7a-495a-e3ec-2d606e429b11"
      },
      "source": [
        "\n",
        "test = pd.read_csv(path_test)\n",
        "ip_test_xt = test['XT'].values \n",
        "ip_test_at = test['AT'].values\n",
        "test_output = test['TOTAL_AT'].values\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x_input=[]\n",
        "y_output =[]\n",
        "\n",
        "data_input2D = []\n",
        "data_output1D = []\n",
        "steps = 3\n",
        "\n",
        "for i in range(0,len(ip_test_xt)-steps+1):\n",
        "  temp_3D = []\n",
        "  for k in range(i, i+steps):\n",
        "    temp_2D = []\n",
        "    temp_2D.append(ip_test_xt[k])\n",
        "    temp_2D.append(ip_test_at[k])\n",
        "    \n",
        "    temp_3D.append(temp_2D)\n",
        "\n",
        "  data_input2D.append(temp_3D)\n",
        "x_input.append(data_input2D)\n",
        "\n",
        "\n",
        "x_input = np.concatenate( x_input, axis=0 )\n",
        "\n",
        "# print(x_input)\n",
        "x_test = x_input.reshape(-1,3,2)\n",
        "# print(x_test.shape)\n",
        "result = model.predict(x_test)\n",
        "\n",
        "test_output = test_output[:result.size]\n",
        "\n",
        "plt.plot(result,'o', color='r')\n",
        "plt.plot(test_output,'x', color='b')\n",
        "plt.title(\"M hnh K60\")\n",
        "plt.xlabel(\"STT\")\n",
        "plt.ylabel(\"Time\")\n",
        "print(\"actually: \",test_output)\n",
        "print(\"model: \", result.reshape(result.size))\n",
        "percent = []\n",
        "for i in range(0,result.size):\n",
        "  percent.append((1- abs(test_output[i]-result[i])/result[i])*100)\n",
        "\n",
        "print(\"pecent: \", percent)\n",
        "print(\"accuracy: \", acc/(result.size))\n",
        "plt.legend(('prediction', 'reality'),loc='upper right')\n",
        "plt.show()\n",
        "acc = 0\n",
        "for i in range(0,result.size):\n",
        "  acc += (1- abs(test_output[i]-result[i])/result[i])*100\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actually:  [21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21]\n",
            "model:  [17.555271 18.347519 18.881042 19.14067  19.110802 19.079414 19.004913\n",
            " 18.836803 18.79916  18.88609  19.096054 19.397816 19.80974  20.273518\n",
            " 20.737309 21.202387]\n",
            "pecent:  [array([80.37781], dtype=float32), array([85.543106], dtype=float32), array([88.77733], dtype=float32), array([90.28597], dtype=float32), array([90.1145], dtype=float32), array([89.93373], dtype=float32), array([89.50226], dtype=float32), array([88.51611], dtype=float32), array([88.29288], dtype=float32), array([88.807045], dtype=float32), array([90.02964], dtype=float32), array([91.74039], dtype=float32), array([93.99154], dtype=float32), array([96.416595], dtype=float32), array([98.73324], dtype=float32), array([99.045456], dtype=float32)]\n",
            "accuracy:  [93.33315]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}